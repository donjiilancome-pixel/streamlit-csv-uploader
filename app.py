# -*- coding: utf-8 -*-
import io, re, hashlib
from io import StringIO
from collections import Counter
from datetime import date, timedelta, time, datetime

import numpy as np
import pandas as pd
import plotly.graph_objects as go
import streamlit as st
from zoneinfo import ZoneInfo
from pandas.api.types import is_datetime64_any_dtype, is_datetime64tz_dtype

# =========================================================
# åŸºæœ¬è¨­å®š
# =========================================================
st.set_page_config(page_title="ãƒ‡ã‚¤ãƒˆãƒ¬çµæœãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰", page_icon="ğŸ“ˆ", layout="wide")
st.title("ğŸ“ˆ ãƒ‡ã‚¤ãƒˆãƒ¬çµæœãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆVWAP/MAå¯¾å¿œãƒ»3åˆ†è¶³ï¼‹IN/OUTï¼‰")

TZ = ZoneInfo("Asia/Tokyo")
MAIN_CHART_HEIGHT = 560
LARGE_CHART_HEIGHT = 820

# å¸‚å ´æ™‚é–“ï¼ˆå‰å ´/å¾Œå ´ï¼‰
MORNING_START_SEC = 9*3600
MORNING_END_SEC   = 11*3600 + 30*60
AFTERNOON_START_SEC = 12*3600 + 30*60
AFTERNOON_END_SEC   = 15*3600 + 30*60

# ç·šè‰²
COLOR_VWAP = "#808080"    # ã‚°ãƒ¬ãƒ¼
COLOR_MA1  = "#2ca02c"    # ç·‘
COLOR_MA2  = "#ff7f0e"    # ã‚ªãƒ¬ãƒ³ã‚¸
COLOR_MA3  = "#1f77b4"    # é’

# =========================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================================================
def _clean_colname(name: str) -> str:
    if name is None: return ""
    s = str(name).replace("\ufeff","").replace("\u3000"," ")
    return re.sub(r"\s+"," ", s.strip())

def clean_columns(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty: return df
    return df.rename(columns={c:_clean_colname(c) for c in df.columns})

def to_numeric_jp(x):
    """æ—¥æœ¬èªCSVã®æ•°å€¤è¡¨è¨˜ã‚’æ•°å€¤åŒ–ã€‚ (123)->-123, å…¨è§’-, æ¡åŒºåˆ‡ã‚Š, å††/æ ª/% é™¤å»"""
    if isinstance(x, pd.Series):
        s = (x.astype(str)
               .str.replace(r"\((\s*[\d,\.]+)\)", r"-\1", regex=True)
               .str.replace("âˆ’", "-", regex=False)
               .str.replace(",", "", regex=False)
               .str.replace("å††", "", regex=False)
               .str.replace("æ ª", "", regex=False)
               .str.replace("%", "", regex=False)
               .str.strip())
        return pd.to_numeric(s, errors="coerce")
    if pd.isna(x): return np.nan
    if isinstance(x, str):
        x = re.sub(r"\((\s*[\d,\.]+)\)", r"-\1", x)
        x = x.replace("âˆ’","-").replace(",","").replace("å††","").replace("æ ª","").replace("%","").strip()
    return pd.to_numeric(x, errors="coerce")

@st.cache_data(show_spinner=False)
def read_table_from_upload(file_name: str, file_bytes: bytes) -> pd.DataFrame:
    # Excel
    if file_name.lower().endswith(".xlsx"):
        try:
            return clean_columns(pd.read_excel(io.BytesIO(file_bytes), engine="openpyxl"))
        except Exception:
            return pd.DataFrame()

    # æ–‡å­—ã‚³ãƒ¼ãƒ‰è‡ªå‹•
    text = None
    for enc in ["utf-8-sig","utf-8","cp932","shift_jis","euc_jp"]:
        try:
            text = file_bytes.decode(enc); break
        except Exception: continue
    if text is None:
        try:
            text = file_bytes.decode("utf-8", errors="replace")
        except Exception:
            return pd.DataFrame()

    # åŒºåˆ‡ã‚Šæ¨å®š
    sample = text[:20000]
    sep = None
    if sample.count("\t") >= 2 and sample.count("\t") > sample.count(","): sep = "\t"
    elif sample.count(";") >= 2 and sample.count(";") > sample.count(","): sep = ";"
    elif sample.count("|") >= 2 and sample.count("|") > sample.count(","): sep = "|"

    try:
        df = pd.read_csv(StringIO(text), sep=sep, engine="python")
        return clean_columns(df)
    except Exception:
        try:
            df = pd.read_csv(StringIO(text), engine="python")
            return clean_columns(df)
        except Exception:
            return pd.DataFrame()

def files_signature(files) -> str:
    if not files: return ""
    parts = []
    for f in files:
        b = f.getvalue()
        h = hashlib.md5(b).hexdigest()
        parts.append(f"{f.name}|{len(b)}|{h[:8]}")
    return hashlib.md5("|".join(parts).encode("utf-8")).hexdigest()

@st.cache_data(show_spinner=False)
def concat_uploaded_tables(files, sig: str, add_source_col: bool=True) -> pd.DataFrame:
    if not files: return pd.DataFrame()
    frames = []
    for f in files:
        df = read_table_from_upload(f.name, f.getvalue())
        if df.empty: continue
        if add_source_col:
            df = df.copy(); df["__source_file__"] = f.name
        frames.append(df)
    return pd.concat(frames, ignore_index=True, sort=False) if frames else pd.DataFrame()

# ---- JSTã®Seriesã«å¼·åˆ¶å¤‰æ›ã™ã‚‹å®‰å…¨ãƒ˜ãƒ«ãƒ‘ãƒ¼
def _to_jst_series(obj, index) -> pd.Series:
    """
    ã©ã‚“ãªå…¥åŠ›ã§ã‚‚ã€å¿…ãš tz-awareï¼ˆJSTï¼‰ã® pandas.Series[datetime64[ns, Asia/Tokyo]] ã‚’è¿”ã™ã€‚
    """
    if isinstance(obj, pd.Series):
        s = pd.to_datetime(obj, errors="coerce", utc=False)
    else:
        s = pd.Series(pd.NaT, index=index, dtype="datetime64[ns]")

    if is_datetime64tz_dtype(s):
        return s.dt.tz_convert(TZ)
    if is_datetime64_any_dtype(s):
        return s.dt.tz_localize(TZ)
    s = pd.to_datetime(s, errors="coerce")
    return s.dt.tz_localize(TZ)

# =========================================================
# éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰/åç§° æ­£è¦åŒ–
# =========================================================
def normalize_symbol_cols(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty: return df
    d = df.copy()

    def extract_code_from_series(s: pd.Series) -> pd.Series:
        if s is None:
            return pd.Series([pd.NA]*len(d), index=d.index, dtype="object")
        ss = s.astype(str).str.strip().str.upper().str.replace(".0","",regex=False)
        s1 = ss.str.extract(r'(?i)(\d{4,5}[A-Z])')[0]
        s2 = ss.str.extract(r'(\d{4,5})')[0]
        return s1.fillna(s2)

    code = pd.Series([pd.NA]*len(d), index=d.index, dtype="object")
    for col in ["ã‚³ãƒ¼ãƒ‰4","éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰","ã‚³ãƒ¼ãƒ‰","ã‚³ãƒ¼ãƒ‰ç•ªå·","ã‚³ãƒ¼ãƒ‰(æ•°å€¤)","ã‚³ãƒ¼ãƒ‰ "]:
        if col in d.columns: code = code.fillna(extract_code_from_series(d[col]))
    for col in ["éŠ˜æŸ„å","åç§°","éŠ˜æŸ„"]:
        if col in d.columns: code = code.fillna(extract_code_from_series(d[col]))
    if "__source_file__" in d.columns:
        sf = d["__source_file__"].astype(str)
        s1 = sf.str.extract(r'_(?i:(\d{4,5}[A-Z]))(?=[,_\.])')[0]
        s2 = sf.str.extract(r'_(\d{4,5})(?=[,_\.])')[0]
        code = code.fillna(s1.fillna(s2))

    d["code_key"] = pd.Series(code, dtype="string").str.upper().str.strip()
    d["code_key"] = d["code_key"].replace({"":"<NA>","NAN":"<NA>"}).replace("<NA>", pd.NA)

    name = None
    for col in ["éŠ˜æŸ„å","åç§°","éŠ˜æŸ„"]:
        if col in d.columns:
            s = d[col].astype(str).str.replace("\u3000"," ", regex=False).str.strip()
            name = s if name is None else name.fillna(s)
    d["name_key"] = name if name is not None else ""
    return d

def representative_name(df: pd.DataFrame) -> str:
    names = [x for x in df["name_key"].dropna().astype(str).tolist() if x and x.lower()!="nan"]
    if not names: return ""
    return Counter(names).most_common(1)[0][0]

def build_code_to_name_map(*dfs: pd.DataFrame) -> dict:
    mp = {}
    for df in dfs:
        if df is None or df.empty: continue
        d = normalize_symbol_cols(df)
        if "code_key" not in d.columns: continue
        grp = d[d["code_key"].notna() & (d["code_key"]!="")]
        if grp.empty: continue
        name_map = grp.groupby("code_key").apply(representative_name)
        for ck, nm in name_map.items():
            if isinstance(ck, str) and ck and nm:
                mp.setdefault(ck, nm)
    return mp

# =========================================================
# å®Ÿç¾æç›Šãƒ»ç´„å®šã®æ­£è¦åŒ–
# =========================================================
def pick_dt_col(df: pd.DataFrame, preferred=None) -> str | None:
    if df is None or df.empty: return None
    cands = preferred or ["ç´„å®šæ—¥","ç´„å®šæ—¥æ™‚","ç´„å®šæ—¥ä»˜","æ—¥æ™‚","æ—¥ä»˜","å¹´æœˆæ—¥","æ±ºæ¸ˆæ—¥","å—æ¸¡æ—¥"]
    for c in cands:
        if c in df.columns: return c
    for c in df.columns:
        if re.search(r"(ç´„å®š|æ±ºæ¸ˆ|å—æ¸¡)?(æ—¥ä»˜|æ—¥æ™‚|å¹´æœˆæ—¥)", str(c)): return c
    return None

def pick_time_col(df: pd.DataFrame, preferred=None) -> str | None:
    if df is None or df.empty: return None
    cands = preferred or ["ç´„å®šæ™‚åˆ»","ç´„å®šæ™‚é–“","æ™‚åˆ»","æ™‚é–“","ç´„å®šæ™‚åˆ»(JST)","ç´„å®šæ™‚é–“(æ—¥æœ¬)","æ™‚é–“(JST)"]
    for c in cands:
        if c in df.columns: return c
    for c in df.columns:
        if re.search(r"(ç´„å®š)?(æ™‚åˆ»|æ™‚é–“)", str(c)): return c
    return None

def parse_time_only_to_timedelta(s: pd.Series) -> pd.Series:
    ss = s.astype(str).str.strip().str.replace("ï¼š",":", regex=False)
    out = pd.Series(pd.NaT, index=ss.index, dtype="timedelta64[ns]")
    as_num = pd.to_numeric(ss, errors="coerce")
    mask_frac = as_num.notna() & (as_num>=0) & (as_num<=1)
    if mask_frac.any():
        secs = (as_num[mask_frac]*86400).round().astype(int)
        out.loc[mask_frac] = pd.to_timedelta(secs, unit="s")
    mask_hms = ss.str.match(r"^\d{1,2}:\d{1,2}(:\d{1,2})?$")
    out.loc[mask_hms] = pd.to_timedelta(ss.loc[mask_hms])
    mask_kanji = ss.str.match(r"^\d{1,2}æ™‚\d{1,2}åˆ†(\d{1,2})?ç§’?$")
    if mask_kanji.any():
        def jp_to_hms(x):
            m = re.match(r"^(\d{1,2})æ™‚(\d{1,2})åˆ†(?:(\d{1,2})ç§’)?$", x)
            hh,mm,ss_ = int(m.group(1)), int(m.group(2)), int(m.group(3) or 0)
            return pd.to_timedelta(f"{hh:02d}:{mm:02d}:{ss_:02d}")
        out.loc[mask_kanji] = ss.loc[mask_kanji].map(jp_to_hms)
    mask_num = ss.str.match(r"^\d{3,6}$")
    if mask_num.any():
        def num_to_hms(x):
            x = x.zfill(6); hh,mm,ss_ = int(x[:2]), int(x[2:4]), int(x[4:6])
            return pd.to_timedelta(f"{hh:02d}:{mm:02d}:{ss_:02d}")
        out.loc[mask_num] = ss.loc[mask_num].map(num_to_hms)
    return out

def combine_date_time_cols(df: pd.DataFrame, date_col: str, time_col: str) -> pd.Series:
    d = pd.to_datetime(df[date_col], errors="coerce", infer_datetime_format=True)
    td = parse_time_only_to_timedelta(df[time_col]) if time_col in df.columns else pd.Series(pd.NaT, index=df.index)
    dt_str = df[date_col].astype(str)
    tail_num = dt_str.str.extract(r"(\d{3,6})\s*$")[0]
    mask_fill = td.isna() & tail_num.notna()
    if mask_fill.any():
        td.loc[mask_fill] = parse_time_only_to_timedelta(tail_num.loc[mask_fill])
    ts = d.dt.floor("D") + td
    ts = pd.to_datetime(ts, errors="coerce")
    try: ts = ts.dt.tz_localize(TZ)
    except Exception: ts = ts.dt.tz_convert(TZ)
    return ts

def parse_datetime_from_dtcol(df: pd.DataFrame, dtcol: str) -> pd.Series:
    s = df[dtcol].astype(str).str.strip().str.replace("ï¼š",":", regex=False)
    date_part = s.str.extract(r"(\d{4}[/-]\d{1,2}[/-]\d{1,2})")[0]
    d = pd.to_datetime(date_part, errors="coerce")
    t_hms  = s.str.extract(r"\b(\d{1,2}:\d{1,2}(?::\d{1,2})?)\b")[0]
    t_kan  = s.str.extract(r"\b(\d{1,2}æ™‚\d{1,2}åˆ†(?:\d{1,2})?ç§’?)\b")[0]
    t_tail = s.str.extract(r"\s(\d{3,6})\s*$")[0]
    t_str = t_hms.fillna(t_kan).fillna(t_tail).fillna("")
    td = parse_time_only_to_timedelta(t_str)
    ts = d.dt.floor("D") + td
    ts = pd.to_datetime(ts, errors="coerce")
    try: ts = ts.dt.tz_localize(TZ)
    except Exception: ts = ts.dt.tz_convert(TZ)
    return ts

def detect_pl_column(d: pd.DataFrame) -> str | None:
    strong = ["å®Ÿç¾æç›Š[å††]","å®Ÿç¾æç›Šï¼ˆå††ï¼‰","å®Ÿç¾æç›Š", "æç›Š[å††]","æç›Šé¡","æç›Š", "å·®å¼•é‡‘é¡","æç›Šåˆè¨ˆ"]
    for c in strong:
        if c in d.columns: return c
    candidates = []
    for c in d.columns:
        s = str(c).lower()
        if any(k in s for k in ["æç›Š","pnl","profit","realized","pl","å·®å¼•"]):
            candidates.append(c)
    if not candidates: return None
    best, best_ratio = None, 0.0
    for c in candidates:
        s = to_numeric_jp(d[c]); ratio = s.notna().mean()
        if ratio > best_ratio: best_ratio, best = ratio, c
    return best

def normalize_realized(df: pd.DataFrame) -> pd.DataFrame:
    """'ç´„å®šæ—¥æ™‚','ç´„å®šæ—¥','å®Ÿç¾æç›Š[å††]' ã‚’ç”Ÿæˆã€‚æ™‚åˆ»ãŒç„¡ã„å ´åˆã¯ 00:00ï¼ˆå¾Œã§æ¨å®šè£œå®Œï¼‰ã€‚"""
    if df is None or df.empty: 
        return df
    d = clean_columns(df.copy())

    # å®Ÿç¾æç›Šåˆ—
    pl_col = detect_pl_column(d)
    d["å®Ÿç¾æç›Š[å††]"] = to_numeric_jp(d[pl_col]) if pl_col else pd.Series(dtype="float64")

    # ç´„å®šæ—¥æ™‚/ç´„å®šæ—¥
    date_col = pick_dt_col(d)
    time_col = pick_time_col(d)

    ts = pd.Series(pd.NaT, index=d.index, dtype="datetime64[ns]")
    if date_col and time_col:
        ts = combine_date_time_cols(d, date_col, time_col)
    elif date_col:
        try:
            ts = pd.to_datetime(d[date_col], errors="coerce", infer_datetime_format=True)
        except Exception:
            for fmt in ("%Y/%m/%d", "%Y-%m-%d", "%Y%m%d"):
                ts = pd.to_datetime(d[date_col], format=fmt, errors="coerce")
                if ts.notna().any(): break
        try: ts = ts.dt.tz_localize(TZ)
        except Exception: ts = ts.dt.tz_convert(TZ)

    d["ç´„å®šæ—¥æ™‚"] = ts
    d["ç´„å®šæ—¥"]  = pd.to_datetime(ts, errors="coerce").dt.date

    # è¿½åŠ æƒ…å ±
    d = normalize_symbol_cols(d)
    if "å£²å´/æ±ºæ¸ˆå˜ä¾¡[å††]" in d.columns: d["__æ±ºæ¸ˆå˜ä¾¡__"] = to_numeric_jp(d["å£²å´/æ±ºæ¸ˆå˜ä¾¡[å††]"])
    if "æ•°é‡[æ ª]" in d.columns:         d["__æ•°é‡__"]   = to_numeric_jp(d["æ•°é‡[æ ª]"])
    d["__action__"] = d.get("å–å¼•", pd.Series(index=d.index, dtype="object"))

    # æ™‚åˆ»ã‚ã‚Šãƒ•ãƒ©ã‚°ï¼ˆ0:00:00ã¯ç„¡ã—æ‰±ã„ï¼‰
    has_time = d["ç´„å®šæ—¥æ™‚"].notna() & ((d["ç´„å®šæ—¥æ™‚"].dt.hour + d["ç´„å®šæ—¥æ™‚"].dt.minute + d["ç´„å®šæ—¥æ™‚"].dt.second) > 0)
    d["ç´„å®šæ™‚åˆ»ã‚ã‚Š"] = has_time.fillna(False)
    return d

def normalize_yakujyou(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty: return df
    return normalize_symbol_cols(df.copy())

# ==== ã“ã“ã‹ã‚‰ï¼šæ™‚åˆ»çµ±åˆã¨æ¨å®šã®é–¢æ•° ====
def pick_dt_with_optional_time(df: pd.DataFrame, dt_candidates=None, tm_candidates=None) -> pd.Series:
    if df is None or df.empty:
        return pd.Series(pd.NaT, index=[], dtype="datetime64[ns, Asia/Tokyo]")
    dtcol = pick_dt_col(df, preferred=dt_candidates)
    tmcol = pick_time_col(df, preferred=tm_candidates)
    if dtcol and tmcol and (tmcol in df.columns):
        ts = combine_date_time_cols(df, dtcol, tmcol)
        return ts
    if dtcol:
        ts = parse_datetime_from_dtcol(df, dtcol)
        return ts
    return _to_jst_series(pd.Series(pd.NaT, index=df.index), df.index)

def attach_exec_time_from_yak(realized_df: pd.DataFrame, yak_df: pd.DataFrame) -> pd.DataFrame:
    if realized_df.empty or yak_df.empty:
        realized_df = realized_df.copy()
        realized_df["ç´„å®šæ—¥æ™‚_æ¨å®š"] = pd.NaT
        return realized_df
    d = realized_df.copy()
    y = normalize_symbol_cols(clean_columns(yak_df.copy()))
    y["ç´„å®šæ—¥æ™‚"] = pick_dt_with_optional_time(y)
    y["__day__"]   = y["ç´„å®šæ—¥æ™‚"].dt.date
    price_col = next((c for c in ["ç´„å®šå˜ä¾¡(å††)","ç´„å®šå˜ä¾¡ï¼ˆå††ï¼‰","ç´„å®šä¾¡æ ¼","ä¾¡æ ¼","ç´„å®šå˜ä¾¡"] if c in y.columns), None)
    qty_col   = next((c for c in ["ç´„å®šæ•°é‡(æ ª/å£)","ç´„å®šæ•°é‡","å‡ºæ¥æ•°é‡","æ•°é‡","æ ªæ•°","å‡ºæ¥é«˜","å£æ•°"] if c in y.columns), None)
    side_col  = next((c for c in ["å£²è²·","å£²è²·åŒºåˆ†","å£²è²·ç¨®åˆ¥","Side","å–å¼•"] if c in y.columns), None)
    if price_col is None:
        for c in y.columns:
            if re.search(r"(ç´„å®š)?.*(å˜ä¾¡|ä¾¡æ ¼)", str(c)): price_col = c; break
    if qty_col is None:
        for c in y.columns:
            if any(k in str(c) for k in ["æ•°é‡","æ ªæ•°","å£æ•°","å‡ºæ¥é«˜"]): qty_col = c; break
    y["__price__"]  = to_numeric_jp(y[price_col]) if price_col else np.nan
    y["__qty__"]    = to_numeric_jp(y[qty_col])   if qty_col else np.nan
    y["__action__"] = y[side_col] if side_col else pd.NA

    d["__day__"] = pd.to_datetime(d.get("ç´„å®šæ—¥", d.get("ç´„å®šæ—¥_final")), errors="coerce").dt.date
    y_grp = y.groupby(["__day__","code_key","__action__"])

    est = []
    matched = 0
    for i, row in d.iterrows():
        if row.get("ç´„å®šæ™‚åˆ»ã‚ã‚Š", False):
            est.append(pd.NaT); continue
        act = row.get("__action__")
        if act not in ("è²·åŸ‹","å£²åŸ‹"):
            est.append(pd.NaT); continue
        key = (row["__day__"], str(row.get("code_key","")).upper(), act)
        if key not in y_grp.groups:
            est.append(pd.NaT); continue
        g = y_grp.get_group(key)
        if g.empty or g["ç´„å®šæ—¥æ™‚"].isna().all():
            est.append(pd.NaT); continue
        tp = row.get("__æ±ºæ¸ˆå˜ä¾¡__", np.nan)
        tq = row.get("__æ•°é‡__", np.nan)
        score = (g["__price__"] - tp).abs()
        if pd.notna(tq):
            score = score + (g["__qty__"] - tq).abs()*0.001
        idx = score.idxmin()
        est_time = g.loc[idx, "ç´„å®šæ—¥æ™‚"]
        est.append(est_time); matched += 1

    d["ç´„å®šæ—¥æ™‚_æ¨å®š"] = pd.Series(est, index=d.index)
    st.caption(f"ğŸ§© å®Ÿç¾æç›Šã«æ™‚åˆ»ã‚’æ¨å®šä»˜ä¸ï¼š{matched} ä»¶ãƒãƒƒãƒï¼ˆè²·åŸ‹/å£²åŸ‹ã®ã¿å¯¾è±¡ï¼‰")
    return d

def enrich_times_lenient(realized_df: pd.DataFrame, yak_df: pd.DataFrame) -> pd.DataFrame:
    if realized_df.empty or yak_df.empty:
        return realized_df
    d = realized_df.copy()
    dt_final = _to_jst_series(d["ç´„å®šæ—¥æ™‚_final"] if "ç´„å®šæ—¥æ™‚_final" in d.columns else None, d.index)
    no_time = dt_final.isna() | ((dt_final.dt.hour==0) & (dt_final.dt.minute==0) & (dt_final.dt.second==0))
    if not no_time.any():
        return d
    if "ç´„å®šæ—¥_final" in d.columns:
        day_base = pd.to_datetime(d["ç´„å®šæ—¥_final"], errors="coerce")
    elif "ç´„å®šæ—¥" in d.columns:
        day_base = pd.to_datetime(d["ç´„å®šæ—¥"], errors="coerce")
    else:
        day_base = _to_jst_series(d.get("ç´„å®šæ—¥æ™‚_final", pd.Series(pd.NaT, index=d.index)), d.index)
    d["__day__"] = (day_base.dt.tz_convert(TZ) if hasattr(day_base.dtype, "tz") else pd.to_datetime(day_base, errors="coerce")).dt.date

    y = normalize_symbol_cols(clean_columns(yak_df.copy()))
    y_dt = pick_dt_with_optional_time(y)
    y = y.assign(__day__=y_dt.dt.date, __dt__=y_dt)

    base = y.dropna(subset=["__dt__"]).copy()
    if base.empty:
        d["ç´„å®šæ—¥æ™‚_final"] = dt_final
        return d

    rep_code = (base.dropna(subset=["code_key"])
                    .groupby(["__day__","code_key"])["__dt__"]
                    .apply(lambda s: s.sort_values().iloc[len(s)//2])
                    .rename("__rep_dt_code__"))
    rep_name = (base.dropna(subset=["name_key"])
                    .groupby(["__day__","name_key"])["__dt__"]
                    .apply(lambda s: s.sort_values().iloc[len(s)//2])
                    .rename("__rep_dt_name__"))
    rep_day = (base.groupby(["__day__"])["__dt__"]
                    .apply(lambda s: s.sort_values().iloc[len(s)//2])
                    .rename("__rep_dt_day__"))

    d["__ck__"] = d.get("code_key", pd.Series([""]*len(d), index=d.index)).astype(str).str.upper()
    d["__nk__"] = d.get("name_key", pd.Series([""]*len(d), index=d.index)).astype(str)

    m = d.merge(rep_code.reset_index(), how="left",
                left_on=["__day__","__ck__"], right_on=["__day__","code_key"])
    fill1 = _to_jst_series(m["__rep_dt_code__"], m.index)
    dt_step1 = dt_final.where(~no_time, fill1)

    still_no = dt_step1.isna() | ((dt_step1.dt.hour==0) & (dt_step1.dt.minute==0) & (dt_step1.dt.second==0))
    m2 = d.merge(rep_name.reset_index(), how="left",
                 left_on=["__day__","__nk__"], right_on=["__day__","name_key"])
    fill2 = _to_jst_series(m2["__rep_dt_name__"], m2.index)
    dt_step2 = dt_step1.where(~still_no, fill2)

    still_no2 = dt_step2.isna() | ((dt_step2.dt.hour==0) & (dt_step2.dt.minute==0) & (dt_step2.dt.second==0))
    m3 = d.merge(rep_day.reset_index(), how="left", on="__day__")
    fill3 = _to_jst_series(m3["__rep_dt_day__"], m3.index)
    dt_final_new = dt_step2.where(~still_no2, fill3)

    d["ç´„å®šæ—¥æ™‚_final"] = dt_final_new
    return d

def session_of(dt_series: pd.Series) -> pd.Series:
    dt_local = _to_jst_series(dt_series, dt_series.index)
    sec = dt_local.dt.hour*3600 + dt_local.dt.minute*60 + dt_local.dt.second
    out = pd.Series(pd.NA, index=dt_series.index, dtype="object")
    out[(sec >= MORNING_START_SEC) & (sec <= MORNING_END_SEC)]  = "å‰å ´"
    out[(sec >= AFTERNOON_START_SEC) & (sec <= AFTERNOON_END_SEC)] = "å¾Œå ´"
    return out

# =========================================================
# 3åˆ†è¶³ãƒ­ãƒ¼ãƒ‰
# =========================================================
@st.cache_data(show_spinner=False)
def load_ohlc_map_from_uploads(files, sig: str):
    ohlc_map = {}
    if not files: return ohlc_map
    for f in files:
        df = read_table_from_upload(f.name, f.getvalue())
        if df.empty: continue

        col_rename_map, found_cols = {}, {}
        CANDIDATES = {
            "time":  ["time","æ—¥æ™‚","date","datetime","timestamp","Time","Date","Datetime","Timestamp","æ—¥ä»˜","æ—¥ä»˜æ™‚åˆ»","æ™‚åˆ»","æ™‚é–“"],
            "open":  ["open","å§‹å€¤","Open","å§‹","O"],
            "high":  ["high","é«˜å€¤","High","é«˜","H"],
            "low":   ["low","å®‰å€¤","Low","å®‰","L"],
            "close": ["close","çµ‚å€¤","Close","çµ‚","C"],
        }
        original_cols = {c.lower(): c for c in df.columns}
        for std, cands in CANDIDATES.items():
            for cand in cands:
                lc = cand.lower()
                if lc in original_cols:
                    col_rename_map[original_cols[lc]] = std
                    found_cols[std] = True
                    break
        if len(found_cols) < len(CANDIDATES):  # å¿…é ˆåˆ—ãŒæƒã‚ãªã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—
            continue

        df = df.rename(columns=col_rename_map)
        t = pd.to_datetime(df["time"], errors="coerce", infer_datetime_format=True)
        t = _to_jst_series(t, t.index)
        df = df.copy(); df["time"] = t

        def pick_one(df, names):
            for n in names:
                if n in df.columns: return df[n]
            return None
        for key, names in [("VWAP",["VWAP","vwap","Vwap"]),
                           ("MA1", ["MA1","ma1","Ma1"]),
                           ("MA2", ["MA2","ma2","Ma2"]),
                           ("MA3", ["MA3","ma3","Ma3"])]:
            s = pick_one(df, names)
            if s is not None: df[key] = pd.to_numeric(s, errors="coerce")

        df = df.dropna(subset=["time"]).sort_values("time")
        df = df.drop_duplicates(subset=["time"], keep="last").reset_index(drop=True)

        key = f.name.rsplit(".",1)[0]
        ohlc_map[key] = df
    return ohlc_map

def extract_code_from_ohlc_key(key: str):
    m = re.search(r'_(?i:(\d{3,5}[a-z]))(?=[,_])', key)
    if m: return m.group(1).upper()
    m2 = re.search(r'_(\d{4,5})(?=[,_])', key)
    if m2: return m2.group(1)
    return None

def guess_name_for_ohlc_key(key: str, code_to_name: dict) -> str | None:
    """
    3åˆ†è¶³ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ code ã‚’æ¨å®šã—ã€å®Ÿç¾æç›Š/ç´„å®šå±¥æ­´ã‹ã‚‰ä½œã£ãŸ CODE_TO_NAME ã§éŠ˜æŸ„åã‚’è¿”ã™ã€‚
    è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æŒ‡æ•°ãƒ»å…ˆç‰©ãªã©ã‚’ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã«å‘½åã€‚
    """
    code = extract_code_from_ohlc_key(key)
    name = None
    if code:
        name = code_to_name.get(code.upper())
    if not name:
        ku = key.upper()
        if "NK2251" in ku or "OSE_NK2251" in ku:
            name = "æ—¥çµŒ225å…ˆç‰©"
        elif "NI225" in ku or "TVC_NI225" in ku:
            name = "æ—¥çµŒå¹³å‡"
    return name

def build_ohlc_code_index(ohlc_map: dict):
    idx = {}
    for k in ohlc_map.keys():
        c = extract_code_from_ohlc_key(k)
        if c:
            idx.setdefault(c.upper(), []).append(k)
    return idx

def ohlc_global_date_range(ohlc_map: dict):
    if not ohlc_map: return None, None
    mins, maxs = [], []
    for df in ohlc_map.values():
        if df is None or df.empty or "time" not in df.columns: continue
        t = _to_jst_series(df["time"], df.index)
        if t.notna().any():
            mins.append(t.min().date()); maxs.append(t.max().date())
    if not mins or not maxs: return None, None
    return min(mins), max(maxs)

def download_button_df(df, label, filename):
    csv = df.to_csv(index=False).encode("utf-8-sig")
    st.download_button(label=label, data=csv, file_name=filename, mime="text/csv")

def compute_max_drawdown(series: pd.Series) -> float:
    if series is None: return np.nan
    s = pd.to_numeric(series, errors="coerce").dropna()
    if s.empty: return np.nan
    arr = s.to_numpy(dtype=float)
    peak, max_dd = -np.inf, 0.0
    for x in arr:
        peak = max(peak, x)
        max_dd = max(max_dd, peak - x)
    return float(max_dd)

# =========================================================
# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ & è¨­å®š
# =========================================================
st.sidebar.header("â‘  ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«OKï¼‰")
realized_files = st.sidebar.file_uploader("å®Ÿç¾æç›Š CSV/Excel", type=["csv","txt","xlsx"], accept_multiple_files=True)
yakujyou_files = st.sidebar.file_uploader("ç´„å®šå±¥æ­´ CSV/Excel", type=["csv","txt","xlsx"], accept_multiple_files=True)
ohlc_files     = st.sidebar.file_uploader("3åˆ†è¶³ OHLC CSV/Excelï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã« _7974 ç­‰ï¼‰", type=["csv","txt","xlsx"], accept_multiple_files=True)

sig_realized = files_signature(realized_files)
sig_yakujyou = files_signature(yakujyou_files)
sig_ohlc     = files_signature(ohlc_files)

yakujyou_all = concat_uploaded_tables(yakujyou_files, sig_yakujyou)
yakujyou_all = normalize_yakujyou(clean_columns(yakujyou_all))

realized = concat_uploaded_tables(realized_files, sig_realized)
realized = normalize_realized(clean_columns(realized))

ohlc_map = load_ohlc_map_from_uploads(ohlc_files, sig_ohlc)
CODE_TO_NAME = build_code_to_name_map(realized, yakujyou_all)

# --- å®Ÿç¾æç›Šã«ç´„å®šå±¥æ­´ã‹ã‚‰æ™‚åˆ»æ¨å®š â†’ æœ€çµ‚åˆ—ã‚’ä½œæˆ
realized = attach_exec_time_from_yak(realized, yakujyou_all)

dt_real = _to_jst_series(realized["ç´„å®šæ—¥æ™‚"]       if "ç´„å®šæ—¥æ™‚" in realized.columns else None, realized.index)
dt_est  = _to_jst_series(realized["ç´„å®šæ—¥æ™‚_æ¨å®š"]   if "ç´„å®šæ—¥æ™‚_æ¨å®š" in realized.columns else None, realized.index)
has_real_clock = dt_real.notna() & ((dt_real.dt.hour + dt_real.dt.minute + dt_real.dt.second) > 0)
realized["ç´„å®šæ—¥æ™‚_final"] = dt_real.where(has_real_clock, dt_est)

# ã‚†ã‚‹ã‚è£œå®Œï¼ˆcodeâ†’nameâ†’dayï¼‰
realized = enrich_times_lenient(realized, yakujyou_all)

# ç´„å®šæ—¥_final
if "ç´„å®šæ—¥" in realized.columns:
    day_raw_date = pd.to_datetime(realized["ç´„å®šæ—¥"], errors="coerce").dt.date
else:
    day_raw_date = pd.Series([pd.NaT]*len(realized), index=realized.index, dtype="object")
realized["ç´„å®šæ—¥_final"] = np.where(pd.notna(day_raw_date),
                                  day_raw_date,
                                  _to_jst_series(realized["ç´„å®šæ—¥æ™‚_final"], realized.index).dt.date)

# ===== è¨ºæ–­ =====
with st.expander("ğŸ›  å®Ÿç¾æç›Š æ­£è¦åŒ–ã®è¨ºæ–­"):
    st.write("è¡Œæ•°:", len(realized))
    if not realized.empty:
        cols = [c for c in ["ç´„å®šæ—¥","ç´„å®šæ—¥_final","ç´„å®šæ—¥æ™‚","ç´„å®šæ—¥æ™‚_æ¨å®š","ç´„å®šæ—¥æ™‚_final",
                            "ç´„å®šæ™‚åˆ»ã‚ã‚Š","å®Ÿç¾æç›Š[å††]","å–å¼•","éŠ˜æŸ„å","éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰","code_key",
                            "__æ±ºæ¸ˆå˜ä¾¡__","__æ•°é‡__"] if c in realized.columns]
        st.write("æ¤œå‡ºåˆ—:", cols)
        st.write(realized[cols].head(12))

# ãƒ•ã‚£ãƒ«ã‚¿
st.sidebar.header("â‘¡ ãƒˆãƒ¬ãƒ¼ãƒ‰ç¨®åˆ¥ãƒ•ã‚£ãƒ«ã‚¿")
trade_type = st.sidebar.radio("å¯¾è±¡", ["å…¨ä½“","ãƒ‡ã‚¤ãƒˆãƒ¬ãƒ¼ãƒ‰ï¼ˆä¸€èˆ¬ï¼‰","ã‚¹ã‚¤ãƒ³ã‚°ãƒˆãƒ¬ãƒ¼ãƒ‰ï¼ˆåˆ¶åº¦ï¼‰"], index=0)

def apply_trade_type_filter(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty or "ä¿¡ç”¨åŒºåˆ†" not in df.columns: return df
    if trade_type.startswith("å…¨ä½“"): return df
    s = df["ä¿¡ç”¨åŒºåˆ†"].astype(str)
    if "ãƒ‡ã‚¤ãƒˆãƒ¬" in trade_type or "ä¸€èˆ¬" in trade_type:
        return df[s.str.contains("ä¸€èˆ¬", na=False)]
    if "ã‚¹ã‚¤ãƒ³ã‚°" in trade_type or "åˆ¶åº¦" in trade_type:
        return df[s.str.contains("åˆ¶åº¦", na=False)]
    return df

# === æ™‚é–“åˆ¥é›†è¨ˆã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ ===
st.sidebar.header("â‘¢ é›†è¨ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³")
drop_noclock = st.sidebar.checkbox("ç´„å®šæ™‚åˆ»ãªã—ã‚’é™¤å¤–ï¼ˆæ™‚é–“åˆ¥ï¼‰", value=True)
include_off_hours = st.sidebar.checkbox("å¸‚å ´æ™‚é–“å¤–ã‚‚å«ã‚ã‚‹ï¼ˆæ™‚é–“åˆ¥ï¼‰", value=False,
                                        help="ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã¨ 9:00ã€œ15:30 ä»¥å¤–ã®æ™‚åˆ»ã‚‚æ™‚é–“åˆ¥é›†è¨ˆã«å«ã‚ã¾ã™")

# æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿
st.subheader("æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿")
c1,c2,c3 = st.columns([2,2,3])
with c1:
    span = st.radio("ã‚¯ã‚¤ãƒƒã‚¯é¸æŠ", ["å…¨æœŸé–“","ä»Šæ—¥","ä»Šé€±","ä»Šæœˆ","ä»Šå¹´","ã‚«ã‚¹ã‚¿ãƒ "], horizontal=True, index=0, key="span")
with c2:
    start = st.date_input("é–‹å§‹æ—¥", value=date.today()-timedelta(days=30), disabled=(span!="ã‚«ã‚¹ã‚¿ãƒ "))
with c3:
    end = st.date_input("çµ‚äº†æ—¥", value=date.today(), disabled=(span!="ã‚«ã‚¹ã‚¿ãƒ "))

def filter_by_span(df, dt_col):
    if df.empty: return df
    if dt_col not in df.columns: return df
    dt = pd.to_datetime(df[dt_col], errors="coerce")
    today = date.today()
    if span=="å…¨æœŸé–“": return df
    if span=="ä»Šæ—¥": s,e = today, today
    elif span=="ä»Šé€±":
        mon = today - timedelta(days=today.weekday()); s,e = mon, mon+timedelta(days=6)
    elif span=="ä»Šæœˆ":
        first = today.replace(day=1)
        next_f = date(first.year+1,1,1) if first.month==12 else date(first.year, first.month+1, 1)
        s,e = first, next_f - timedelta(days=1)
    elif span=="ä»Šå¹´":
        s,e = date(today.year,1,1), date(today.year,12,31)
    else:
        s,e = start, end
    mask = (dt.dt.date>=s) & (dt.dt.date<=e)
    return df.loc[mask]

realized_f = apply_trade_type_filter(filter_by_span(realized, "ç´„å®šæ—¥_final"))

# =========================================================
# KPI
# =========================================================
st.subheader("KPI")
if not realized_f.empty and "å®Ÿç¾æç›Š[å††]" in realized_f.columns:
    realized_f = realized_f.copy()
    realized_f["å®Ÿç¾æç›Š[å††]"] = to_numeric_jp(realized_f["å®Ÿç¾æç›Š[å††]"])

pl = None
if not realized_f.empty and "å®Ÿç¾æç›Š[å††]" in realized_f.columns:
    pl = to_numeric_jp(realized_f["å®Ÿç¾æç›Š[å††]"]).dropna()

c1,c2,c3 = st.columns(3)
with c1:
    total_pl = pl.sum() if pl is not None and not pl.empty else np.nan
    st.metric("å®Ÿç¾æç›Šï¼ˆé¸æŠæœŸé–“ï¼‰", f"{int(total_pl):,} å††" if pd.notna(total_pl) else "â€”")
with c2:
    n_trades = int(pl.shape[0]) if pl is not None else 0
    st.metric("å–å¼•å›æ•°", f"{n_trades:,}" if n_trades else "â€”")
with c3:
    avg_pl = pl.mean() if pl is not None and not pl.empty else np.nan
    st.metric("å¹³å‡æç›Šï¼ˆ/å›ï¼‰", f"{int(round(avg_pl)):,} å††" if pd.notna(avg_pl) else "â€”")

# =========================================================
# ã‚¿ãƒ–
# =========================================================
tab1, tab1b, tab2, tab3, tab4, tab5 = st.tabs([
    "é›†è¨ˆï¼ˆæœŸé–“åˆ¥ï¼‰",
    "é›†è¨ˆï¼ˆæ™‚é–“åˆ¥ï¼‰",
    "ç´¯è¨ˆæç›Š",
    "å€‹åˆ¥éŠ˜æŸ„",
    "ãƒ©ãƒ³ã‚­ãƒ³ã‚°",
    "3åˆ†è¶³ IN/OUT + æŒ‡æ¨™"
])

# ---- 1) æœŸé–“åˆ¥
with tab1:
    st.markdown("### å®Ÿç¾æç›Šï¼ˆæœŸé–“åˆ¥é›†è¨ˆï¼‰")
    if realized_f.empty or "å®Ÿç¾æç›Š[å††]" not in realized_f.columns:
        st.info("å®Ÿç¾æç›Šãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã€‚")
    else:
        r = realized_f.copy()
        r = r[r["å®Ÿç¾æç›Š[å††]"].notna()]
        if r.empty:
            st.info("å®Ÿç¾æç›Šã®æ•°å€¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        else:
            r["æ—¥"] = pd.to_datetime(r["ç´„å®šæ—¥_final"], errors="coerce").dt.date
            r["é€±"] = (pd.to_datetime(r["æ—¥"]) - pd.to_timedelta(pd.to_datetime(r["æ—¥"]).dt.weekday, unit="D")).dt.date
            r["æœˆ"] = pd.to_datetime(r["æ—¥"]).dt.to_period("M").dt.to_timestamp().dt.date
            r["å¹´"] = pd.to_datetime(r["æ—¥"]).dt.to_period("Y").dt.to_timestamp().dt.date

            for label,col in [("æ—¥åˆ¥","æ—¥"),("é€±åˆ¥","é€±"),("æœˆåˆ¥","æœˆ"),("å¹´åˆ¥","å¹´")]:
                g = r.groupby(col, as_index=False)["å®Ÿç¾æç›Š[å††]"].sum().sort_values(col)
                # è¡¨ç¤ºæ•´å½¢ï¼šæ•°å€¤ã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã€æ—¥ä»˜ã¯ YYYY-MM-DD æ–‡å­—åˆ—
                disp = g.copy()
                disp["æ—¥ä»˜"] = pd.to_datetime(disp[col]).dt.strftime("%Y-%m-%d")
                disp["å®Ÿç¾æç›Š[å††]"] = disp["å®Ÿç¾æç›Š[å††]"].round(0).astype("Int64").map(lambda x: f"{x:,}")
                st.write(f"**{label}**")
                st.dataframe(disp[["æ—¥ä»˜","å®Ÿç¾æç›Š[å††]"]], use_container_width=True, hide_index=True)
                download_button_df(disp[["æ—¥ä»˜","å®Ÿç¾æç›Š[å††]"]], f"â¬‡ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆ{label}ï¼‰", f"{col}_pl.csv")

                # ã‚°ãƒ©ãƒ•ï¼šXè»¸ã¯æ™‚é–“è¡¨è¨˜ãªã—ã® 'YYYY-MM-DD'
                fig_bar = go.Figure([go.Bar(x=disp["æ—¥ä»˜"], y=g["å®Ÿç¾æç›Š[å††]"], name=f"{label} å®Ÿç¾æç›Š")])
                fig_bar.update_layout(margin=dict(l=10,r=10,t=20,b=10), height=300,
                                      xaxis_title="æ—¥ä»˜", yaxis_title="å®Ÿç¾æç›Š[å††]")
                st.plotly_chart(fig_bar, use_container_width=True)

# ---- 1b) æ™‚é–“åˆ¥
def pick_best_exec_time_series(df: pd.DataFrame, index=None) -> pd.Series:
    """finalâ†’æ¨å®šâ†’å…ƒ ã®é †ã§â€œæ™‚åˆ»ã‚ã‚Šâ€ã ã‘ã‚’æ¡ç”¨ï¼ˆJST, tz-awareï¼‰"""
    idx = df.index if index is None else index
    base = pd.Series(pd.NaT, index=idx, dtype="datetime64[ns, Asia/Tokyo]")
    def fill_from(colname: str, base: pd.Series) -> pd.Series:
        if colname not in df.columns: return base
        s = _to_jst_series(df[colname], df.index)
        has_clock = s.notna() & ((s.dt.hour + s.dt.minute + s.dt.second) > 0)
        return base.mask(base.isna() & has_clock, s)
    base = fill_from("ç´„å®šæ—¥æ™‚_final", base)
    base = fill_from("ç´„å®šæ—¥æ™‚_æ¨å®š", base)
    base = fill_from("ç´„å®šæ—¥æ™‚",     base)
    return base

with tab1b:
    st.markdown("### å®Ÿç¾æç›Šï¼ˆæ™‚é–“åˆ¥ãƒ»1æ™‚é–“ã”ã¨ï¼‰")
    if realized_f.empty or "å®Ÿç¾æç›Š[å††]" not in realized_f.columns:
        st.info("å®Ÿç¾æç›Šãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã€‚")
    else:
        d0 = realized_f.copy()
        d0 = d0[d0["å®Ÿç¾æç›Š[å††]"].notna()]
        if d0.empty:
            st.info("å®Ÿç¾æç›Šã®æ•°å€¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        else:
            # finalâ†’æ¨å®šâ†’å…ƒ ã®é †ã§â€œæ™‚åˆ»ã‚ã‚Šâ€ã ã‘ã‚’æ¡ç”¨ï¼ˆJST, tz-awareï¼‰
            dt0 = pick_best_exec_time_series(d0, index=d0.index)

            # è¨ºæ–­
            def count_has_clock(col):
                if col not in d0.columns: return 0
                s = _to_jst_series(d0[col], d0.index)
                return int((s.notna() & ((s.dt.hour + s.dt.minute + s.dt.second) > 0)).sum())
            with st.expander("ğŸ§ª æ™‚é–“åˆ¥ é›†è¨ˆã®è¨ºæ–­", expanded=False):
                st.write("å€™è£œåˆ¥â€œæ™‚åˆ»ã‚ã‚Šâ€ä»¶æ•°ï¼š",
                         {"ç´„å®šæ—¥æ™‚_final": count_has_clock("ç´„å®šæ—¥æ™‚_final"),
                          "ç´„å®šæ—¥æ™‚_æ¨å®š": count_has_clock("ç´„å®šæ—¥æ™‚_æ¨å®š"),
                          "ç´„å®šæ—¥æ™‚":     count_has_clock("ç´„å®šæ—¥æ™‚")})
                st.write("dt0 ã‚µãƒ³ãƒ—ãƒ«ï¼ˆå…ˆé ­5ä»¶ï¼‰:")
                st.write(pd.DataFrame({"dt0": dt0}).head())

            # ã€ŒçœŸã«æ™‚åˆ»ã‚ã‚Šã€
            hh0, mm0, ss0 = dt0.dt.hour, dt0.dt.minute, dt0.dt.second
            has_clock0 = dt0.notna() & ((hh0.fillna(0)*3600 + mm0.fillna(0)*60 + ss0.fillna(0)) > 0)

            cnt_all      = len(d0)
            cnt_time     = int(has_clock0.sum())
            cnt_midnight = int(((dt0.notna()) & ~has_clock0).sum())
            st.caption(f"â±ï¸ çœŸã«æ™‚åˆ»ã‚ã‚Š: {cnt_time}/{cnt_all} | 00:00æ‰±ã„: {cnt_midnight}")

            # â‘  ç´„å®šæ™‚åˆ»ãªã—ã‚’é™¤å¤–
            if drop_noclock:
                d, dt = d0.loc[has_clock0].copy(), dt0.loc[has_clock0]
            else:
                d, dt = d0.copy(), dt0.copy()

            if d.empty:
                st.info("â€œç´„å®šæ™‚åˆ»ã‚ã‚Šâ€ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆ00:00ã‚„NaTã¯é™¤å¤–ï¼‰ã€‚")
            else:
                # â‘¡ å¸‚å ´æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿
                if include_off_hours:
                    d_in, dt_in = d, dt
                    info_suffix = "ï¼ˆå¸‚å ´æ™‚é–“å¤–ã‚‚å«ã‚€ï¼‰"
                else:
                    sec = (dt.dt.hour*3600 + dt.dt.minute*60 + dt.dt.second).astype(int)
                    mask_mkt = (sec >= MORNING_START_SEC) & (sec <= AFTERNOON_END_SEC)
                    d_in, dt_in = d.loc[mask_mkt].copy(), dt.loc[mask_mkt]
                    info_suffix = ""

                if d_in.empty:
                    st.info("å¸‚å ´æ™‚é–“å†…ã®â€œç´„å®šæ™‚åˆ»ã‚ã‚Šâ€ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚" + ("ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§å¸‚å ´æ™‚é–“å¤–ã‚’å«ã‚ã‚‰ã‚Œã¾ã™ï¼‰" if not include_off_hours else ""))
                else:
                    d_in["PL"]  = to_numeric_jp(d_in["å®Ÿç¾æç›Š[å††]"])
                    d_in["win"] = d_in["PL"] > 0

                    hour_floor = dt_in.dt.floor("H")
                    hour_x = pd.to_datetime([datetime(2000,1,1,h.hour,0,0, tzinfo=TZ) for h in hour_floor])
                    d_in["hour_x"] = hour_x

                    x_range = [datetime(2000,1,1,9,0, tzinfo=TZ), datetime(2000,1,1,15,30, tzinfo=TZ)]
                    x_ticks = pd.date_range(x_range[0], x_range[1], freq="60min", inclusive="both")
                    base = pd.DataFrame({"hour_x": x_ticks})

                    by = d_in.groupby("hour_x", as_index=False).agg(
                        åæ”¯=("PL","sum"),
                        å–å¼•å›æ•°=("PL","count"),
                        å‹ç‡=("win","mean"),
                        å¹³å‡æç›Š=("PL","mean")
                    )
                    by = base.merge(by, on="hour_x", how="left")

                    # è¡¨ç¤ºæ•´å½¢ï¼ˆã”è¦æœ›ï¼‰
                    disp = by.copy()
                    disp["æ™‚é–“"] = disp["hour_x"].dt.strftime("%H:%M")
                    disp["åæ”¯"] = disp["åæ”¯"].fillna(0).round(0).astype("Int64").map(lambda x: f"{x:,}")
                    disp["å–å¼•å›æ•°"] = disp["å–å¼•å›æ•°"].fillna(0).astype("Int64")
                    disp["å‹ç‡"] = (disp["å‹ç‡"].fillna(0)*100).round(1).map(lambda x: f"{x:.1f}%")
                    disp["å¹³å‡æç›Š"] = disp["å¹³å‡æç›Š"].fillna(0).round(0).astype("Int64").map(lambda x: f"{x:,}")
                    st.dataframe(disp[["æ™‚é–“","åæ”¯","å–å¼•å›æ•°","å‹ç‡","å¹³å‡æç›Š"]],
                                 use_container_width=True, hide_index=True)
                    download_button_df(disp[["æ™‚é–“","åæ”¯","å–å¼•å›æ•°","å‹ç‡","å¹³å‡æç›Š"]],
                                       "â¬‡ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæ™‚é–“åˆ¥ï¼‰", "hourly_stats.csv")

                    # ã‚°ãƒ©ãƒ•ã¯æ•°å€¤ã®ã¾ã¾ï¼ˆXè»¸ã¯æ™‚åˆ»è¡¨ç¤ºï¼‰
                    fig_h_pl = go.Figure([go.Bar(x=by["hour_x"], y=by["åæ”¯"], name="åæ”¯ï¼ˆåˆè¨ˆï¼‰")])
                    fig_h_pl.update_layout(title=f"æ™‚é–“åˆ¥ åæ”¯ï¼ˆåˆè¨ˆï¼‰{info_suffix}", xaxis_title="æ™‚é–“", yaxis_title="å††",
                                           margin=dict(l=10,r=10,t=30,b=10), height=300,
                                           xaxis=dict(tickformat="%H:%M", range=x_range))
                    st.plotly_chart(fig_h_pl, use_container_width=True)

                    fig_h_wr = go.Figure([go.Scatter(x=by["hour_x"], y=by["å‹ç‡"]*100, mode="lines+markers", name="å‹ç‡")])
                    fig_h_wr.update_layout(title=f"æ™‚é–“åˆ¥ å‹ç‡ï¼ˆå…¨ä½“ï¼‰{info_suffix}", xaxis_title="æ™‚é–“", yaxis_title="å‹ç‡ï¼ˆ%ï¼‰",
                                           margin=dict(l=10,r=10,t=30,b=10), height=300,
                                           yaxis=dict(range=[0,100]),
                                           xaxis=dict(tickformat="%H:%M", range=x_range))
                    st.plotly_chart(fig_h_wr, use_container_width=True)

                    fig_h_cnt = go.Figure([go.Bar(x=by["hour_x"], y=by["å–å¼•å›æ•°"], name="å–å¼•å›æ•°")])
                    fig_h_cnt.update_layout(title=f"æ™‚é–“åˆ¥ å–å¼•å›æ•°{info_suffix}", xaxis_title="æ™‚é–“", yaxis_title="å›",
                                            margin=dict(l=10,r=10,t=30,b=10), height=300,
                                            xaxis=dict(tickformat="%H:%M", range=x_range))
                    st.plotly_chart(fig_h_cnt, use_container_width=True)

                    fig_h_avg = go.Figure([go.Bar(x=by["hour_x"], y=by["å¹³å‡æç›Š"], name="å¹³å‡æç›Šï¼ˆ/å›ï¼‰")])
                    fig_h_avg.update_layout(title=f"æ™‚é–“åˆ¥ å¹³å‡æç›Šï¼ˆ/å›ï¼‰{info_suffix}", xaxis_title="æ™‚é–“", yaxis_title="å††/å›",
                                            margin=dict(l=10,r=10,t=30,b=10), height=300,
                                            xaxis=dict(tickformat="%H:%M", range=x_range))
                    st.plotly_chart(fig_h_avg, use_container_width=True)

                    # å‰å ´ / å¾Œå ´ æ¯”è¼ƒ
                    st.markdown("### å‰å ´ / å¾Œå ´ æ¯”è¼ƒ" + info_suffix)
                    ses = session_of(dt_in)
                    d_in["ã‚»ãƒƒã‚·ãƒ§ãƒ³"] = ses
                    cmp = d_in.dropna(subset=["ã‚»ãƒƒã‚·ãƒ§ãƒ³"]).groupby("ã‚»ãƒƒã‚·ãƒ§ãƒ³").agg(
                        åæ”¯=("PL","sum"), å–å¼•å›æ•°=("PL","count"),
                        å‹ç‡=("win","mean"), å¹³å‡æç›Š=("PL","mean")
                    ).reset_index()
                    cmp_disp = cmp.copy()
                    cmp_disp["åæ”¯"] = cmp_disp["åæ”¯"].round(0).astype("Int64").map(lambda x: f"{x:,}")
                    cmp_disp["å–å¼•å›æ•°"] = cmp_disp["å–å¼•å›æ•°"].astype("Int64")
                    cmp_disp["å‹ç‡"] = (cmp_disp["å‹ç‡"]*100).round(1).map(lambda x: f"{x:.1f}%")
                    cmp_disp["å¹³å‡æç›Š"] = cmp_disp["å¹³å‡æç›Š"].round(0).astype("Int64").map(lambda x: f"{x:,}")
                    st.dataframe(cmp_disp, use_container_width=True, hide_index=True)
                    download_button_df(cmp_disp, "â¬‡ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå‰å ´å¾Œå ´æ¯”è¼ƒï¼‰", "am_pm_compare.csv")

                    # ç´¯ç©å‹ç‡ï¼ˆ5åˆ†ï¼‰
                    st.markdown("### ç´¯ç©å‹ç‡ã®æ™‚é–“æ¨ç§»ï¼ˆå…¨æœŸé–“ãƒ»5åˆ†ãƒ“ãƒ³ï¼‰" + info_suffix)
                    five = dt_in.dt.floor("5min")
                    x_five = pd.to_datetime([datetime(2000,1,1,t.hour,t.minute,0, tzinfo=TZ) for t in five.dt.time])
                    tmp = pd.DataFrame({"x": x_five, "win": d_in["win"].astype(float), "cnt": 1.0})
                    grid = pd.DataFrame({"x": pd.date_range(datetime(2000,1,1,9,0, tzinfo=TZ),
                                                            datetime(2000,1,1,15,30, tzinfo=TZ),
                                                            freq="5min", inclusive="both")})
                    agg5 = tmp.groupby("x").agg(win_sum=("win","sum"), cnt=("cnt","sum")).reset_index()
                    grid = grid.merge(agg5, on="x", how="left").fillna(0.0)
                    grid["cum_wr"] = np.where(grid["cnt"].cumsum()>0,
                                              grid["win_sum"].cumsum()/grid["cnt"].cumsum()*100.0, np.nan)
                    fig_cum = go.Figure([go.Scatter(x=grid["x"], y=grid["cum_wr"], mode="lines", name="ç´¯ç©å‹ç‡")])
                    fig_cum.update_layout(title=f"ç´¯ç©å‹ç‡ï¼ˆæ™‚é–“ã®çµŒéã¨ã¨ã‚‚ã«ï¼‰{info_suffix}", xaxis_title="æ™‚é–“", yaxis_title="å‹ç‡ï¼ˆ%ï¼‰",
                                          margin=dict(l=10,r=10,t=30,b=10), height=320,
                                          yaxis=dict(range=[0,100]),
                                          xaxis=dict(tickformat="%H:%M",
                                                     range=[datetime(2000,1,1,9,0, tzinfo=TZ),
                                                            datetime(2000,1,1,15,30, tzinfo=TZ)]))
                    st.plotly_chart(fig_cum, use_container_width=True)

# ---- 2) ç´¯è¨ˆæç›Š
with tab2:
    st.markdown("### ç´¯è¨ˆå®Ÿç¾æç›Šï¼ˆé¸æŠæœŸé–“å†…ã€æ—¥æ¬¡ãƒ™ãƒ¼ã‚¹ï¼‰")
    if realized_f.empty or "å®Ÿç¾æç›Š[å††]" not in realized_f.columns:
        st.info("å®Ÿç¾æç›Šãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã€‚")
    else:
        d = realized_f.copy()
        d = d[d["å®Ÿç¾æç›Š[å††]"].notna()]
        if d.empty:
            st.info("å®Ÿç¾æç›Šã®æ•°å€¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        else:
            d["æ—¥"] = pd.to_datetime(d["ç´„å®šæ—¥_final"], errors="coerce").dt.date
            seq = d.groupby("æ—¥", as_index=False)["å®Ÿç¾æç›Š[å††]"].sum().sort_values("æ—¥")
            seq["ç´¯è¨ˆ"] = pd.to_numeric(seq["å®Ÿç¾æç›Š[å††]"], errors="coerce").cumsum()

            # è¡¨ç¤ºæ•´å½¢ï¼šãƒ†ãƒ¼ãƒ–ãƒ«ã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã€ã‚°ãƒ©ãƒ•ã®Xã¯YYYY-MM-DDæ–‡å­—åˆ—
            seq_disp = seq.copy()
            seq_disp["æ—¥ä»˜"] = pd.to_datetime(seq_disp["æ—¥"]).dt.strftime("%Y-%m-%d")
            seq_disp["å®Ÿç¾æç›Š[å††]"] = seq_disp["å®Ÿç¾æç›Š[å††]"].round(0).astype("Int64").map(lambda x: f"{x:,}")
            seq_disp["ç´¯è¨ˆ"] = seq_disp["ç´¯è¨ˆ"].round(0).astype("Int64").map(lambda x: f"{x:,}")
            st.dataframe(seq_disp[["æ—¥ä»˜","å®Ÿç¾æç›Š[å††]","ç´¯è¨ˆ"]], use_container_width=True, hide_index=True)
            download_button_df(seq_disp[["æ—¥ä»˜","å®Ÿç¾æç›Š[å††]","ç´¯è¨ˆ"]], "â¬‡ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆç´¯è¨ˆãƒ»æ—¥æ¬¡ï¼‰", "cumulative_daily_pl.csv")

            left,right = st.columns(2)
            with left:
                fig_bar = go.Figure([go.Bar(x=seq_disp["æ—¥ä»˜"], y=seq["å®Ÿç¾æç›Š[å††]"], name="æ—¥æ¬¡ å®Ÿç¾æç›Š")])
                fig_bar.update_layout(margin=dict(l=10,r=10,t=20,b=10), height=350, xaxis_title="æ—¥ä»˜", yaxis_title="å®Ÿç¾æç›Š[å††]")
                st.plotly_chart(fig_bar, use_container_width=True)
            with right:
                fig_line = go.Figure([go.Scatter(x=seq_disp["æ—¥ä»˜"], y=seq["ç´¯è¨ˆ"], mode="lines", name="ç´¯è¨ˆ")])
                fig_line.update_layout(margin=dict(l=10,r=10,t=20,b=10), height=350, xaxis_title="æ—¥ä»˜", yaxis_title="ç´¯è¨ˆå®Ÿç¾æç›Š[å††]")
                st.plotly_chart(fig_line, use_container_width=True)

# ---- 3) å€‹åˆ¥éŠ˜æŸ„
def per_symbol_stats(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame(columns=["éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰","éŠ˜æŸ„å","å®Ÿç¾æç›Šåˆè¨ˆ","å–å¼•å›æ•°","1å›å¹³å‡æç›Š","å‹ç‡"])
    d = normalize_symbol_cols(df.copy())
    if "å®Ÿç¾æç›Š[å††]" in d.columns:
        d["å®Ÿç¾æç›Š"] = to_numeric_jp(d["å®Ÿç¾æç›Š[å††]"])
    else:
        cand = next((c for c in d.columns if ("å®Ÿç¾" in str(c) and "æç›Š" in str(c))), None)
        if cand is None: 
            return pd.DataFrame(columns=["éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰","éŠ˜æŸ„å","å®Ÿç¾æç›Šåˆè¨ˆ","å–å¼•å›æ•°","1å›å¹³å‡æç›Š","å‹ç‡"])
        d["å®Ÿç¾æç›Š"] = to_numeric_jp(d[cand])
    d = d[d["å®Ÿç¾æç›Š"].notna()]
    if d.empty:
        return pd.DataFrame(columns=["éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰","éŠ˜æŸ„å","å®Ÿç¾æç›Šåˆè¨ˆ","å–å¼•å›æ•°","1å›å¹³å‡æç›Š","å‹ç‡"])
    d["win"] = d["å®Ÿç¾æç›Š"]>0
    d["group_key"] = np.where(d["code_key"].notna()&(d["code_key"]!=""), d["code_key"], "NAMEONLY::"+d["name_key"].astype(str))
    agg = d.groupby("group_key").agg({"å®Ÿç¾æç›Š":["sum","count","mean"], "win":["mean"]})
    agg.columns = ["å®Ÿç¾æç›Šåˆè¨ˆ","å–å¼•å›æ•°","1å›å¹³å‡æç›Š","å‹ç‡"]
    rep_name = d.groupby("group_key").apply(representative_name).rename("éŠ˜æŸ„å")
    code_col = d.groupby("group_key")["code_key"].first().rename("éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰")
    out = agg.join(rep_name).join(code_col).reset_index(drop=True).sort_values("å®Ÿç¾æç›Šåˆè¨ˆ", ascending=False)
    out["éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰"] = out["éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰"].fillna("â€”")
    return out

with tab3:
    st.markdown("### å€‹åˆ¥éŠ˜æŸ„ï¼ˆå‹ç‡ãƒ»å®Ÿç¾æç›Šï¼‰")
    if realized_f.empty or "å®Ÿç¾æç›Š[å††]" not in realized_f.columns:
        st.info("å®Ÿç¾æç›Šãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã€‚")
    else:
        sym = per_symbol_stats(realized_f)
        if not sym.empty:
            disp = sym.copy()
            if "å®Ÿç¾æç›Šåˆè¨ˆ" in disp.columns: disp["å®Ÿç¾æç›Šåˆè¨ˆ"] = disp["å®Ÿç¾æç›Šåˆè¨ˆ"].round(0).astype("Int64").map(lambda x: f"{x:,}")
            if "1å›å¹³å‡æç›Š" in disp.columns: disp["1å›å¹³å‡æç›Š"] = disp["1å›å¹³å‡æç›Š"].round(0).astype("Int64").map(lambda x: f"{x:,}")
            if "å‹ç‡" in disp.columns: disp["å‹ç‡"] = (disp["å‹ç‡"]*100).round(1).map(lambda x: f"{x:.1f}%")
            order = ["éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰","éŠ˜æŸ„å","å®Ÿç¾æç›Šåˆè¨ˆ","å–å¼•å›æ•°","1å›å¹³å‡æç›Š","å‹ç‡"]
            cols = [c for c in order if c in disp.columns] + [c for c in disp.columns if c not in order]
            st.dataframe(disp[cols], use_container_width=True, hide_index=True)
            download_button_df(disp[cols], "â¬‡ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå€‹åˆ¥éŠ˜æŸ„ï¼‰", "per_symbol_stats.csv")
        else:
            st.info("é›†è¨ˆã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

# ---- 4) ãƒ©ãƒ³ã‚­ãƒ³ã‚°
with tab4:
    st.markdown("### ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆé¸æŠæœŸé–“ï¼‰")
    if realized_f.empty or "å®Ÿç¾æç›Š[å††]" not in realized_f.columns:
        st.info("å®Ÿç¾æç›Šãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã€‚")
    else:
        d = normalize_symbol_cols(realized_f.copy())
        d["å®Ÿç¾æç›Š"] = to_numeric_jp(d["å®Ÿç¾æç›Š[å††]"]) if "å®Ÿç¾æç›Š[å††]" in d.columns else np.nan
        d = d[d["å®Ÿç¾æç›Š"].notna()]
        if d.empty:
            st.info("å®Ÿç¾æç›Šã®æ•°å€¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        else:
            d["group_key"] = np.where(d["code_key"].notna()&(d["code_key"]!=""), d["code_key"], "NAMEONLY::"+d["name_key"].astype(str))
            by_symbol = d.groupby("group_key").agg({"å®Ÿç¾æç›Š":["count","sum","mean"]})
            by_symbol.columns = ["å–å¼•å›æ•°","å®Ÿç¾æç›Šåˆè¨ˆ","1å›å¹³å‡æç›Š"]
            rep_name = d.groupby("group_key").apply(representative_name).rename("éŠ˜æŸ„å")
            code_col = d.groupby("group_key")["code_key"].first().rename("éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰")
            out = by_symbol.join(rep_name).join(code_col).reset_index(drop=True)
            out["éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰"] = out["éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰"].fillna("â€”")
            if "1å›å¹³å‡æç›Š" in out.columns: out["1å›å¹³å‡æç›Š"] = out["1å›å¹³å‡æç›Š"].round(0).astype("Int64").map(lambda x: f"{x:,}")
            if "å®Ÿç¾æç›Šåˆè¨ˆ" in out.columns: out["å®Ÿç¾æç›Šåˆè¨ˆ"] = out["å®Ÿç¾æç›Šåˆè¨ˆ"].round(0).astype("Int64").map(lambda x: f"{x:,}")
            left,right = st.columns(2)
            with left:
                sort_key = st.selectbox("ã‚½ãƒ¼ãƒˆæŒ‡æ¨™", ["å®Ÿç¾æç›Šåˆè¨ˆ","å–å¼•å›æ•°","1å›å¹³å‡æç›Š"], index=0)
            with right:
                topn = st.slider("è¡¨ç¤ºä»¶æ•°", 5, 100, 20)
            out = out.sort_values(sort_key, ascending=False).head(topn)
            order = ["éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰","éŠ˜æŸ„å","å®Ÿç¾æç›Šåˆè¨ˆ","å–å¼•å›æ•°","1å›å¹³å‡æç›Š"]
            cols = [c for c in order if c in out.columns] + [c for c in out.columns if c not in order]
            st.dataframe(out[cols], use_container_width=True, hide_index=True)
            download_button_df(out[cols], "â¬‡ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼‰", "ranking.csv")

# =========================================================
# 5) 3åˆ†è¶³ IN/OUT + æŒ‡æ¨™ï¼ˆå…ˆã«æ—¥ä»˜ã‚’é¸ã³ã€ãã®æ—¥ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹éŠ˜æŸ„ã ã‘é¸æŠï¼‰
# =========================================================

# --- æ—¢ã«å®šç¾©æ¸ˆã¿ã§ã‚ã‚Œã°å†å®šç¾©ã—ãªã„ãƒ˜ãƒ«ãƒ‘ãƒ¼ ---
try:
    guess_name_for_ohlc_key
except NameError:
    def guess_name_for_ohlc_key(key: str, code_to_name: dict) -> str | None:
        code = extract_code_from_ohlc_key(key)
        name = None
        if code:
            name = code_to_name.get(str(code).upper())
        if not name:
            ku = key.upper()
            if "NK2251" in ku or "OSE_NK2251" in ku:
                name = "æ—¥çµŒ225å…ˆç‰©"
            elif "NI225" in ku or "TVC_NI225" in ku:
                name = "æ—¥çµŒå¹³å‡"
        return name

# --- IN/OUTã‚’4åˆ†é¡ï¼ˆè²·å»º/å£²å»º/å£²åŸ‹/è²·åŸ‹ï¼‰ã§è¿‘å‚ãƒãƒ¼ã¸ã‚¹ãƒŠãƒƒãƒ— ---
def align_trades_to_ohlc(ohlc: pd.DataFrame, trades: pd.DataFrame, max_gap_min=6):
    """ç´„å®šï¼ˆè²·å»º/å£²å»º/å£²åŸ‹/è²·åŸ‹ï¼‰ã‚’OHLCã®æœ€ã‚‚è¿‘ã„ãƒãƒ¼ã«çµã³ã¤ã‘ã‚‹ã€‚"""
    if ohlc is None or ohlc.empty or trades is None or trades.empty:
        return pd.DataFrame(columns=["time","price","side","qty","label4"])

    tdf = trades.copy()
    tdf["ç´„å®šæ—¥æ™‚"] = _to_jst_series(tdf["ç´„å®šæ—¥æ™‚"] if "ç´„å®šæ—¥æ™‚" in tdf.columns else None, tdf.index)

    price_col = next((c for c in ["ç´„å®šå˜ä¾¡(å††)","ç´„å®šå˜ä¾¡ï¼ˆå††ï¼‰","ç´„å®šä¾¡æ ¼","ä¾¡æ ¼","ç´„å®šå˜ä¾¡"] if c in tdf.columns), None)
    qty_col   = next((c for c in ["ç´„å®šæ•°é‡(æ ª/å£)","ç´„å®šæ•°é‡","å‡ºæ¥æ•°é‡","æ•°é‡","æ ªæ•°","å‡ºæ¥é«˜","å£æ•°"] if c in tdf.columns), None)
    side_col  = next((c for c in ["å£²è²·","å£²è²·åŒºåˆ†","å£²è²·ç¨®åˆ¥","Side","å–å¼•"] if c in tdf.columns), None)
    if price_col is None:
        for c in tdf.columns:
            if re.search(r"(ç´„å®š)?.*(å˜ä¾¡|ä¾¡æ ¼)", str(c)): price_col = c; break
    if qty_col is None:
        for c in tdf.columns:
            if any(k in str(c) for k in ["æ•°é‡","æ ªæ•°","å£æ•°","å‡ºæ¥é«˜"]): qty_col = c; break

    tdf["price"] = to_numeric_jp(tdf[price_col]) if price_col else np.nan
    tdf["qty"]   = to_numeric_jp(tdf[qty_col])   if qty_col else np.nan
    tdf["side"]  = tdf[side_col].astype(str) if side_col else ""

    def classify_side4(s: str) -> str | None:
        s = str(s)
        if "è²·å»º" in s: return "è²·å»º"
        if "å£²å»º" in s: return "å£²å»º"
        if "å£²åŸ‹" in s: return "å£²åŸ‹"
        if "è²·åŸ‹" in s: return "è²·åŸ‹"
        # è‹±èªãƒ»è¨˜å·ã®ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯
        if ("è²·" in s and ("æ–°è¦" in s or "å»º" in s)) or re.search(r"\bBUY\b.*\b(OPEN|NEW)\b", s, re.I): return "è²·å»º"
        if ("å£²" in s and ("æ–°è¦" in s or "å»º" in s)) or re.search(r"\bSELL\b.*\b(OPEN|NEW)\b", s, re.I): return "å£²å»º"
        if ("å£²" in s and ("è¿”æ¸ˆ" in s or "æ±ºæ¸ˆ" in s)) or re.search(r"\bSELL\b.*\b(CLOSE)\b", s, re.I): return "å£²åŸ‹"
        if ("è²·" in s and ("è¿”æ¸ˆ" in s or "æ±ºæ¸ˆ" in s)) or re.search(r"\bBUY\b.*\b(CLOSE|COVER)\b", s, re.I): return "è²·åŸ‹"
        return None

    tdf["label4"] = tdf["side"].map(classify_side4)

    odf = ohlc.copy()
    tt = _to_jst_series(odf["time"], odf.index)
    odf = odf.set_index(tt)

    out_rows = []
    for _, row in tdf.iterrows():
        t0 = row["ç´„å®šæ—¥æ™‚"]
        if pd.isna(t0) or not row.get("label4"):
            continue
        lo = t0 - pd.Timedelta(minutes=max_gap_min)
        hi = t0 + pd.Timedelta(minutes=max_gap_min)
        window = odf.loc[lo:hi]
        if window.empty:
            continue
        idx = (window.index - t0).abs().argmin()
        near_time = window.index[idx]
        price_on_bar = window.loc[near_time, "close"]
        out_rows.append({
            "time": near_time, "price": price_on_bar,
            "side": row["side"], "qty": row["qty"], "label4": row["label4"]
        })
    return pd.DataFrame(out_rows)

# --- ãƒ­ãƒ¼ã‚½ã‚¯ï¼‹æŒ‡æ¨™ï¼ˆæ™‚é–“ãƒ¬ãƒ³ã‚¸ï¼†é«˜ã•ã‚’å¤–ã‹ã‚‰æŒ‡å®šå¯ï¼‰ ---
def make_candle_with_indicators(df: pd.DataFrame, title="", height=560, x_range=None):
    fig = go.Figure()
    fig.add_trace(go.Candlestick(
        x=df["time"], open=df["open"], high=df["high"], low=df["low"], close=df["close"],
        name="OHLC", showlegend=False
    ))
    if "VWAP" in df.columns and df["VWAP"].notna().any():
        fig.add_trace(go.Scatter(x=df["time"], y=df["VWAP"], name="VWAP", mode="lines",
                                 line=dict(color=COLOR_VWAP, width=1.3)))
    if "MA1" in df.columns and df["MA1"].notna().any():
        fig.add_trace(go.Scatter(x=df["time"], y=df["MA1"], name="MA1", mode="lines",
                                 line=dict(color=COLOR_MA1, width=1.3)))
    if "MA2" in df.columns and df["MA2"].notna().any():
        fig.add_trace(go.Scatter(x=df["time"], y=df["MA2"], name="MA2", mode="lines",
                                 line=dict(color=COLOR_MA2, width=1.3)))
    if "MA3" in df.columns and df["MA3"].notna().any():
        fig.add_trace(go.Scatter(x=df["time"], y=df["MA3"], name="MA3", mode="lines",
                                 line=dict(color=COLOR_MA3, width=1.3)))

    fig.update_layout(
        title=title, height=height, margin=dict(l=10, r=10, t=40, b=10),
        xaxis_rangeslider_visible=False,
        xaxis=dict(showgrid=False, range=x_range),
        yaxis=dict(showgrid=True)
    )
    return fig

with tab5:
    st.markdown("### 3åˆ†è¶³ IN/OUT + æŒ‡æ¨™ï¼ˆVWAP/MA1/MA2/MA3ï¼‰")
    if not ohlc_map:
        st.info("3åˆ†è¶³OHLCãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    else:
        # ã¾ãšå…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªæ—¥ä»˜ç¯„å›²ã‚’æ¨å®š
        dmin, dmax = ohlc_global_date_range(ohlc_map)
        if dmin is None or dmax is None:
            st.info("æœ‰åŠ¹ãªæ—¥æ™‚åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            c1, c2, c3 = st.columns([2,2,1])
            with c1:
                sel_date = st.date_input("è¡¨ç¤ºæ—¥ã‚’é¸æŠ", value=dmin, min_value=dmin, max_value=dmax)
            with c2:
                enlarge = st.toggle("ğŸ” æ‹¡å¤§è¡¨ç¤º", value=False, help="ãƒã‚§ãƒƒã‚¯ã§ãƒãƒ£ãƒ¼ãƒˆã‚’å¤§ããã—ã¾ã™")
            with c3:
                ht = LARGE_CHART_HEIGHT if enlarge else MAIN_CHART_HEIGHT

            # è¡¨ç¤ºæ™‚é–“ãƒ¬ãƒ³ã‚¸ï¼ˆå›ºå®š 9:00ã€œ15:30ï¼‰
            t0 = pd.Timestamp(f"{sel_date} 09:00", tz=TZ)
            t1 = pd.Timestamp(f"{sel_date} 15:30", tz=TZ)
            x_range = [t0, t1]

            # é¸æŠæ—¥ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹éŠ˜æŸ„ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚­ãƒ¼ï¼‰ã ã‘ã‚’æç¤º
            keys_that_day = []
            for k, df in ohlc_map.items():
                if df is None or df.empty: 
                    continue
                vw = df[(df["time"]>=t0) & (df["time"]<=t1)]
                if not vw.empty:
                    keys_that_day.append(k)

            if not keys_that_day:
                st.info("é¸æŠæ—¥ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹éŠ˜æŸ„ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆ¥ã®æ—¥ä»˜ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚")
            else:
                # ãƒ©ãƒ™ãƒ«ã«éŠ˜æŸ„åã‚’è¡¨ç¤º
                options = sorted(keys_that_day)
                def _fmt_label(k):
                    nm = guess_name_for_ohlc_key(k, CODE_TO_NAME)
                    return f"{k}ï¼ˆ{nm}ï¼‰" if nm else k

                sel_key = st.selectbox("éŠ˜æŸ„ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åï¼‰ã‚’é¸æŠ", options=options, index=0, format_func=_fmt_label)
                sel_name = guess_name_for_ohlc_key(sel_key, CODE_TO_NAME)
                if sel_name:
                    st.caption(f"æƒ³å®šéŠ˜æŸ„å: **{sel_name}**")

                view = ohlc_map[sel_key]
                view = view[(view["time"]>=t0) & (view["time"]<=t1)].copy()
                if view.empty:
                    st.info(f"{sel_key}ï¼š{sel_date} ã®3åˆ†è¶³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                else:
                    # è©²å½“ã‚³ãƒ¼ãƒ‰ã®ç´„å®šã‚’å½“æ—¥æŠ½å‡º
                    yak = yakujyou_all.copy()
                    if "code_key" in yak.columns:
                        this_code = extract_code_from_ohlc_key(sel_key) or ""
                        if this_code:
                            yak = yak[yak["code_key"].astype(str).str.upper()==this_code.upper()]
                    y_dtcol = pick_dt_col(yak) or "ç´„å®šæ—¥"
                    yak = yak.copy()
                    yak["ç´„å®šæ—¥æ™‚"] = pick_dt_with_optional_time(yak) if y_dtcol in yak.columns else _to_jst_series(pd.Series(pd.NaT, index=yak.index), yak.index)
                    yak = yak[yak["ç´„å®šæ—¥æ™‚"].notna()]
                    yak = yak[(yak["ç´„å®šæ—¥æ™‚"]>=t0) & (yak["ç´„å®šæ—¥æ™‚"]<=t1)]

                    # è¿‘å‚ãƒãƒ¼ã¸ã‚¹ãƒŠãƒƒãƒ—ï¼ˆè²·å»º/å£²å»º/å£²åŸ‹/è²·åŸ‹ï¼‰
                    trades = align_trades_to_ohlc(view, yak, max_gap_min=6) if not yak.empty else pd.DataFrame(columns=["time","price","side","qty","label4"])

                    # ãƒ¡ã‚¤ãƒ³ãƒãƒ£ãƒ¼ãƒˆï¼ˆã‚µã‚¤ã‚ºçµ±ä¸€ãƒ»æ™‚é–“å›ºå®šï¼‰
                    title_text = f"{sel_name} [{sel_key}]" if sel_name else sel_key
                    fig = make_candle_with_indicators(view, title=title_text, height=ht, x_range=x_range)

                    # 4åˆ†é¡ãƒãƒ¼ã‚«ãƒ¼
                    marker_styles = {
                        "è²·å»º": dict(symbol="triangle-up",        size=11, color="#2ca02c", line=dict(width=1.2)),
                        "å£²å»º": dict(symbol="triangle-down",      size=11, color="#d62728", line=dict(width=1.2)),
                        "å£²åŸ‹": dict(symbol="triangle-down-open", size=12, color="#9467bd", line=dict(width=1.4)),
                        "è²·åŸ‹": dict(symbol="triangle-up-open",   size=12, color="#1f77b4", line=dict(width=1.4)),
                    }
                    if not trades.empty:
                        for label, mk in marker_styles.items():
                            sub = trades[trades["label4"] == label]
                            if not sub.empty:
                                fig.add_trace(go.Scatter(
                                    x=sub["time"], y=sub["price"], mode="markers",
                                    name=label, marker=mk,
                                    hovertemplate=f"{label}<br>%{{x|%H:%M}}<br>Â¥%{{y:.0f}}<extra></extra>"
                                ))

                    st.plotly_chart(fig, use_container_width=True, config={"displayModeBar": True})

                # åŒæ—¥ã®ä¸‹ã«ï¼šæ—¥çµŒå…ˆç‰© / æ—¥çµŒå¹³å‡ï¼ˆæ™‚é–“ãƒ¬ãƒ³ã‚¸å›ºå®šãƒ»ã‚µã‚¤ã‚ºçµ±ä¸€ï¼‰
                # å€™è£œã‚­ãƒ¼ï¼ˆå½“æ—¥ãƒ‡ãƒ¼ã‚¿ã‚ã‚Šã«é™å®šï¼‰
                fut_keys = [k for k in keys_that_day if ("NK2251" in k or "OSE_NK2251" in k)]
                idx_keys = [k for k in keys_that_day if ("NI225" in k or "TVC_NI225" in k)]

                st.markdown("#### æ—¥çµŒå…ˆç‰©ï¼ˆNK225miniç­‰ï¼‰")
                if fut_keys:
                    k = fut_keys[0]
                    df = ohlc_map.get(k)
                    vw = df[(df["time"]>=t0) & (df["time"]<=t1)].copy()
                    if not vw.empty:
                        nm = guess_name_for_ohlc_key(k, CODE_TO_NAME)
                        ttl = f"{nm} [{k}]" if nm else k
                        figx = make_candle_with_indicators(vw, title=ttl, height=ht, x_range=x_range)
                        st.plotly_chart(figx, use_container_width=True, config={"displayModeBar": True})
                    else:
                        st.info(f"{k}ï¼š{sel_date} ã®ãƒ‡ãƒ¼ã‚¿ãªã—ã€‚")
                else:
                    st.info("æ—¥çµŒå…ˆç‰©ï¼ˆ`OSE_NK2251!` ãªã©ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

                st.markdown("#### æ—¥çµŒå¹³å‡")
                if idx_keys:
                    k = idx_keys[0]
                    df = ohlc_map.get(k)
                    vw = df[(df["time"]>=t0) & (df["time"]<=t1)].copy()
                    if not vw.empty:
                        nm = guess_name_for_ohlc_key(k, CODE_TO_NAME)
                        ttl = f"{nm} [{k}]" if nm else k
                        figx = make_candle_with_indicators(vw, title=ttl, height=ht, x_range=x_range)
                        st.plotly_chart(figx, use_container_width=True, config={"displayModeBar": True})
                    else:
                        st.info(f"{k}ï¼š{sel_date} ã®ãƒ‡ãƒ¼ã‚¿ãªã—ã€‚")
                else:
                    st.info("æ—¥çµŒå¹³å‡ï¼ˆ`TVC_NI225` ãªã©ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
