# -*- coding: utf-8 -*-
import io, re, hashlib
from io import StringIO
from collections import Counter
from datetime import date, timedelta, time, datetime

import numpy as np
import pandas as pd
import plotly.graph_objects as go
import streamlit as st
from zoneinfo import ZoneInfo

# =========================================================
# åŸºæœ¬è¨­å®š
# =========================================================
st.set_page_config(page_title="ãƒ‡ã‚¤ãƒˆãƒ¬çµæœãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰", page_icon="ğŸ“ˆ", layout="wide")
st.title("ğŸ“ˆ ãƒ‡ã‚¤ãƒˆãƒ¬çµæœãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆVWAP/MAå¯¾å¿œãƒ»3åˆ†è¶³ï¼‹IN/OUTï¼‰")
st.caption("è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾å¿œãƒ»3åˆ†è¶³: Asia/Tokyo / 9:00â€“15:30ãƒ»ä¿¡ç”¨åŒºåˆ†ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆå…¨ä½“/ä¸€èˆ¬=ãƒ‡ã‚¤/åˆ¶åº¦=ã‚¹ã‚¤ãƒ³ã‚°ï¼‰")

TZ = ZoneInfo("Asia/Tokyo")
MAIN_CHART_HEIGHT = 600   # æ¨™æº–ã®é«˜ã•
LARGE_CHART_HEIGHT = 860  # æ‹¡å¤§è¡¨ç¤ºæ™‚ã®é«˜ã•

# æ™‚é–“å¸¯ï¼ˆå‰å ´/å¾Œå ´ï¼‰
MORNING_START_SEC = 9*3600
MORNING_END_SEC   = 11*3600 + 30*60
AFTERNOON_START_SEC = 12*3600 + 30*60
AFTERNOON_END_SEC   = 15*3600 + 30*60

# =========================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================================================
def _clean_colname(name: str) -> str:
    if name is None: return ""
    s = str(name).replace("\ufeff","").replace("\u3000"," ")
    return re.sub(r"\s+"," ", s.strip())

def clean_columns(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty: return df
    return df.rename(columns={c:_clean_colname(c) for c in df.columns})

def to_numeric_jp(x):
    """æ—¥æœ¬èªCSVã§ã‚ˆãã‚ã‚‹è¡¨è¨˜ã‚’æ•°å€¤åŒ–ã€‚ (123)â†’-123, å…¨è§’ãƒã‚¤ãƒŠã‚¹, æ¡åŒºåˆ‡ã‚Š, å††/æ ª/ï¼…ãªã©é™¤å»"""
    if isinstance(x, pd.Series):
        s = (x.astype(str)
               .str.replace(r"\((\s*[\d,\.]+)\)", r"-\1", regex=True)
               .str.replace("âˆ’", "-", regex=False)
               .str.replace(",", "", regex=False)
               .str.replace("å††", "", regex=False)
               .str.replace("æ ª", "", regex=False)
               .str.replace("%", "", regex=False)
               .str.strip())
        return pd.to_numeric(s, errors="coerce")
    if pd.isna(x): return np.nan
    if isinstance(x, str):
        x = re.sub(r"\((\s*[\d,\.]+)\)", r"-\1", x)
        x = x.replace("âˆ’","-").replace(",","").replace("å††","").replace("æ ª","").replace("%","").strip()
    return pd.to_numeric(x, errors="coerce")

# ---- ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ã®å …ç‰¢ãƒªãƒ¼ãƒ€ãƒ¼ï¼ˆCSV/TXT/XLSXï¼‰
@st.cache_data(show_spinner=False)
def read_table_from_upload(file_name: str, file_bytes: bytes) -> pd.DataFrame:
    # Excel
    if file_name.lower().endswith(".xlsx"):
        try:
            return clean_columns(pd.read_excel(io.BytesIO(file_bytes), engine="openpyxl"))
        except Exception:
            return pd.DataFrame()

    # ãƒ†ã‚­ã‚¹ãƒˆï¼ˆCSV/TSV/ãã®ä»–åŒºåˆ‡ã‚Šï¼‰
    text = None
    for enc in ["utf-8-sig","utf-8","cp932","shift_jis","euc_jp"]:
        try:
            text = file_bytes.decode(enc)
            break
        except Exception:
            continue
    if text is None:
        try:
            text = file_bytes.decode("utf-8", errors="replace")
        except Exception:
            return pd.DataFrame()

    # åŒºåˆ‡ã‚Šæ¨å®š
    sample = text[:20000]
    sep = None
    if sample.count("\t") >= 2 and sample.count("\t") > sample.count(","): sep = "\t"
    elif sample.count(";") >= 2 and sample.count(";") > sample.count(","): sep = ";"
    elif sample.count("|") >= 2 and sample.count("|") > sample.count(","): sep = "|"

    try:
        df = pd.read_csv(StringIO(text), sep=sep, engine="python")
        return clean_columns(df)
    except Exception:
        try:
            df = pd.read_csv(StringIO(text), engine="python")
            return clean_columns(df)
        except Exception:
            return pd.DataFrame()

def files_signature(files) -> str:
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
    if not files: return ""
    parts = []
    for f in files:
        b = f.getvalue()
        h = hashlib.md5(b).hexdigest()
        parts.append(f"{f.name}|{len(b)}|{h[:8]}")
    return hashlib.md5("|".join(parts).encode("utf-8")).hexdigest()

@st.cache_data(show_spinner=False)
def concat_uploaded_tables(files, sig: str, add_source_col: bool=True) -> pd.DataFrame:
    if not files: return pd.DataFrame()
    frames = []
    for f in files:
        df = read_table_from_upload(f.name, f.getvalue())
        if df.empty: continue
        if add_source_col:
            df = df.copy(); df["__source_file__"] = f.name
        frames.append(df)
    return pd.concat(frames, ignore_index=True, sort=False) if frames else pd.DataFrame()

# =========================================================
# éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰/åç§° æ­£è¦åŒ–
# =========================================================
def normalize_symbol_cols(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty: return df
    d = df.copy()

    def extract_code_from_series(s: pd.Series) -> pd.Series:
        if s is None:
            return pd.Series([pd.NA]*len(d), index=d.index, dtype="object")
        ss = s.astype(str).str.strip().str.upper().str.replace(".0","",regex=False)
        s1 = ss.str.extract(r'(?i)(\d{4,5}[A-Z])')[0]
        s2 = ss.str.extract(r'(\d{4,5})')[0]
        return s1.fillna(s2)

    code = pd.Series([pd.NA]*len(d), index=d.index, dtype="object")
    for col in ["ã‚³ãƒ¼ãƒ‰4","éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰","ã‚³ãƒ¼ãƒ‰"]:
        if col in d.columns: code = code.fillna(extract_code_from_series(d[col]))
    for col in ["éŠ˜æŸ„å","åç§°","éŠ˜æŸ„"]:
        if col in d.columns: code = code.fillna(extract_code_from_series(d[col]))
    if "__source_file__" in d.columns:
        sf = d["__source_file__"].astype(str)
        s1 = sf.str.extract(r'_(?i:(\d{4,5}[A-Z]))(?=[,_\.])')[0]
        s2 = sf.str.extract(r'_(\d{4,5})(?=[,_\.])')[0]
        code = code.fillna(s1.fillna(s2))

    d["code_key"] = pd.Series(code, dtype="string").str.upper().str.strip()
    d["code_key"] = d["code_key"].replace({"":"<NA>","NAN":"<NA>"}).replace("<NA>", pd.NA)

    name = None
    for col in ["éŠ˜æŸ„å","åç§°","éŠ˜æŸ„"]:
        if col in d.columns:
            s = d[col].astype(str).str.replace("\u3000"," ", regex=False).str.strip()
            name = s if name is None else name.fillna(s)
    d["name_key"] = name if name is not None else ""
    return d

def representative_name(df: pd.DataFrame) -> str:
    names = [x for x in df["name_key"].dropna().astype(str).tolist() if x and x.lower()!="nan"]
    if not names: return ""
    return Counter(names).most_common(1)[0][0]

def build_code_to_name_map(*dfs: pd.DataFrame) -> dict:
    mp = {}
    for df in dfs:
        if df is None or df.empty: continue
        d = normalize_symbol_cols(df)
        if "code_key" not in d.columns: continue
        grp = d[d["code_key"].notna() & (d["code_key"]!="")]
        if grp.empty: continue
        name_map = grp.groupby("code_key").apply(representative_name)
        for ck, nm in name_map.items():
            if isinstance(ck, str) and ck and nm:
                mp.setdefault(ck, nm)
    return mp

# =========================================================
# å®Ÿç¾æç›Šãƒ»ç´„å®šã®æ­£è¦åŒ–
# =========================================================
def pick_dt_col(df: pd.DataFrame, preferred=None) -> str | None:
    if df is None or df.empty: return None
    cands = preferred or ["ç´„å®šæ—¥","ç´„å®šæ—¥æ™‚","ç´„å®šæ—¥ä»˜","æ—¥æ™‚","æ—¥ä»˜"]
    for c in cands:
        if c in df.columns: return c
    for c in df.columns:
        if re.search(r"(ç´„å®š)?(æ—¥ä»˜|æ—¥æ™‚)", str(c)): return c
    return None

def pick_time_col(df: pd.DataFrame, preferred=None) -> str | None:
    if df is None or df.empty: return None
    cands = preferred or ["ç´„å®šæ™‚åˆ»","ç´„å®šæ™‚é–“","æ™‚åˆ»","æ™‚é–“","ç´„å®šæ™‚åˆ»(JST)","ç´„å®šæ™‚é–“(æ—¥æœ¬)","æ™‚é–“(JST)"]
    for c in cands:
        if c in df.columns: return c
    for c in df.columns:
        if re.search(r"(ç´„å®š)?(æ™‚åˆ»|æ™‚é–“)", str(c)): return c
    return None

def _contains_time_string(s: pd.Series) -> pd.Series:
    ss = s.astype(str)
    has_hms = ss.str.contains(r"\d{1,2}[:ï¼š]\d{1,2}")
    has_num = ss.str.contains(r"\b\d{3,6}\b")
    has_jp  = ss.str.contains(r"\d{1,2}æ™‚\d{1,2}åˆ†")
    return has_hms | has_num | has_jp

def parse_time_only_to_timedelta(s: pd.Series) -> pd.Series:
    ss = s.astype(str).str.strip().str.replace("ï¼š",":", regex=False)
    out = pd.Series(pd.NaT, index=ss.index, dtype="timedelta64[ns]")
    as_num = pd.to_numeric(ss, errors="coerce")
    mask_frac = as_num.notna() & (as_num>=0) & (as_num<=1)
    if mask_frac.any():
        secs = (as_num[mask_frac]*86400).round().astype(int)
        out.loc[mask_frac] = pd.to_timedelta(secs, unit="s")
    mask_hms = ss.str.match(r"^\d{1,2}:\d{1,2}(:\d{1,2})?$")
    out.loc[mask_hms] = pd.to_timedelta(ss.loc[mask_hms])
    mask_kanji = ss.str.match(r"^\d{1,2}æ™‚\d{1,2}åˆ†(\d{1,2}ç§’)?$")
    if mask_kanji.any():
        def jp_to_hms(x):
            m = re.match(r"^(\d{1,2})æ™‚(\d{1,2})åˆ†(?:(\d{1,2})ç§’)?$", x)
            hh,mm,ss_ = int(m.group(1)), int(m.group(2)), int(m.group(3) or 0)
            return pd.to_timedelta(f"{hh:02d}:{mm:02d}:{ss_:02d}")
        out.loc[mask_kanji] = ss.loc[mask_kanji].map(jp_to_hms)
    mask_num = ss.str.match(r"^\d{3,6}$")
    if mask_num.any():
        def num_to_hms(x):
            x = x.zfill(6); hh,mm,ss_ = int(x[:2]), int(x[2:4]), int(x[4:6])
            return pd.to_timedelta(f"{hh:02d}:{mm:02d}:{ss_:02d}")
        out.loc[mask_num] = ss.loc[mask_num].map(num_to_hms)
    return out

def combine_date_time_cols(df: pd.DataFrame, date_col: str, time_col: str) -> pd.Series:
    d = pd.to_datetime(df[date_col], errors="coerce", infer_datetime_format=True)
    td = parse_time_only_to_timedelta(df[time_col]) if time_col in df.columns else pd.Series(pd.NaT, index=df.index)
    # æ—¥ä»˜ãŒæ–‡å­—åˆ—æœ«å°¾ã«æ™‚åˆ»æ•°å€¤ã‚’å«ã‚€ã‚±ãƒ¼ã‚¹ã¸ã®æ•‘æ¸ˆ
    dt_str = df[date_col].astype(str)
    tail_num = dt_str.str.extract(r"(\d{3,6})\s*$")[0]
    mask_fill = td.isna() & tail_num.notna()
    if mask_fill.any():
        td.loc[mask_fill] = parse_time_only_to_timedelta(tail_num.loc[mask_fill])
    ts = d.dt.floor("D") + td
    ts = pd.to_datetime(ts, errors="coerce")
    try:
        ts = ts.dt.tz_localize(TZ)
    except Exception:
        ts = ts.dt.tz_convert(TZ)
    return ts

def parse_datetime_from_dtcol(df: pd.DataFrame, dtcol: str) -> pd.Series:
    s = df[dtcol].astype(str).str.strip().str.replace("ï¼š",":", regex=False)
    date_part = s.str.extract(r"(\d{4}[/-]\d{1,2}[/-]\d{1,2})")[0]
    d = pd.to_datetime(date_part, errors="coerce")
    t_hms  = s.str.extract(r"\b(\d{1,2}:\d{1,2}(?::\d{1,2})?)\b")[0]
    t_kan  = s.str.extract(r"\b(\d{1,2}æ™‚\d{1,2}åˆ†(?:\d{1,2}ç§’)?)\b")[0]
    t_tail = s.str.extract(r"\s(\d{3,6})\s*$")[0]
    t_str = t_hms.fillna(t_kan).fillna(t_tail).fillna("")
    td = parse_time_only_to_timedelta(t_str)
    ts = d.dt.floor("D") + td
    ts = pd.to_datetime(ts, errors="coerce")
    try:
        ts = ts.dt.tz_localize(TZ)
    except Exception:
        ts = ts.dt.tz_convert(TZ)
    return ts

def normalize_realized(df: pd.DataFrame) -> pd.DataFrame:
    """
    åˆ—åã‚†ã‚‹æ¤œå‡ºï¼‹æ•°å€¤åŒ–ã€‚'ç´„å®šæ—¥æ™‚'(TZä»˜ã) ã¨ 'ç´„å®šæ—¥'(date) ã¨ 'å®Ÿç¾æç›Š[å††]' ã‚’ä½œã‚‹ã€‚
    ã•ã‚‰ã« 'ç´„å®šæ™‚åˆ»ã‚ã‚Š'(bool) ã‚’ä»˜ä¸ã—ã€æ™‚åˆ»æƒ…å ±ã‚’æŒãŸãªã„è¡Œã‚’é™¤å¤–ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
    """
    if df is None or df.empty:
        return df
    d = clean_columns(df.copy())

    # å€™è£œæ¤œå‡º
    date_col = pick_dt_col(d)
    time_col = pick_time_col(d)

    # --- ç´„å®šæ—¥æ™‚ã®ç”Ÿæˆ ---
    if date_col and time_col:
        ts = combine_date_time_cols(d, date_col, time_col)
        has_time_raw = d[time_col].astype(str).str.strip().ne("")
    elif date_col and _contains_time_string(d[date_col]).any():
        ts = parse_datetime_from_dtcol(d, date_col)
        has_time_raw = _contains_time_string(d[date_col])
    elif date_col:
        ts = pd.to_datetime(d[date_col], errors="coerce", infer_datetime_format=True)
        try:
            ts = ts.dt.tz_localize(TZ)
        except Exception:
            ts = ts.dt.tz_convert(TZ)
        has_time_raw = pd.Series(False, index=d.index)
    else:
        ts = pd.Series(pd.NaT, index=d.index, dtype="datetime64[ns]")
        has_time_raw = pd.Series(False, index=d.index)

    # TZä»˜ä¸ï¼ˆå†ä¿è¨¼ï¼‰
    if getattr(ts.dtype, "tz", None) is None:
        try:
            ts = pd.to_datetime(ts, errors="coerce").dt.tz_localize(TZ)
        except Exception:
            ts = pd.to_datetime(ts, errors="coerce").dt.tz_convert(TZ)

    # "ç´„å®šæ™‚åˆ»ã‚ã‚Š": å®Ÿéš›ã«æ™‚åˆ†ç§’ãŒ 00:00:00 ä»¥å¤–
    time_nonzero = ts.notna() & ((ts.dt.hour + ts.dt.minute + ts.dt.second) > 0)
    d["ç´„å®šæ—¥æ™‚"] = ts
    d["ç´„å®šæ—¥"] = pd.to_datetime(ts, errors="coerce").dt.date
    d["ç´„å®šæ™‚åˆ»ã‚ã‚Š"] = (has_time_raw | time_nonzero).fillna(False)

    # å®Ÿç¾æç›Šåˆ—ã‚’æ¤œå‡º â†’ "å®Ÿç¾æç›Š[å††]" ã«æ­£è¦åŒ–
    pl_col = None
    for c in ["å®Ÿç¾æç›Š[å††]","å®Ÿç¾æç›Šï¼ˆå††ï¼‰","å®Ÿç¾æç›Š","æç›Š[å††]","æç›Šé¡","æç›Š"]:
        if c in d.columns:
            pl_col = c; break
    if pl_col is None:
        candidates = [c for c in d.columns if any(t in str(c).lower() for t in ["æç›Š","pnl","profit","realized","pl"])]
        best, best_ratio = None, 0.0
        for c in candidates:
            s = to_numeric_jp(d[c])
            ratio = s.notna().mean()
            if ratio > best_ratio:
                best_ratio, best = ratio, c
        pl_col = best
    d["å®Ÿç¾æç›Š[å††]"] = to_numeric_jp(d[pl_col]) if pl_col else pd.Series(dtype="float64")

    return normalize_symbol_cols(d)

def normalize_yakujyou(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty: return df
    d = normalize_symbol_cols(df.copy())
    return d

def lr_from_realized_trade(val: str) -> str | None:
    if val is None or (isinstance(val, float) and pd.isna(val)): return None
    s = str(val).replace("ã€€","").replace(" ","")
    if "å£²åŸ‹" in s or ("å£²" in s and "è¿”æ¸ˆ" in s): return "LONG"
    if "è²·åŸ‹" in s or ("è²·" in s and "è¿”æ¸ˆ" in s): return "SHORT"
    if "è²·å»º" in s or "å£²å»º" in s: return None
    sl = s.lower()
    if "sell" in sl and "close" in sl: return "LONG"
    if "buy" in sl and "close" in sl:  return "SHORT"
    return None

def side_to_io(val: str) -> str | None:
    if val is None or (isinstance(val,float) and np.isnan(val)): return None
    s = str(val).replace("ã€€","").replace(" ","").lower()
    if "è²·å»º" in s or ("buy" in s and "close" not in s):  return "IN"
    if "å£²å»º" in s or ("sell" in s and "close" not in s): return "IN"
    if "å£²åŸ‹" in s or "å£²è¿”æ¸ˆ" in s or ("sell" in s and "close" in s): return "OUT"
    if "è²·åŸ‹" in s or "è²·è¿”æ¸ˆ" in s or ("buy" in s and "close" in s):  return "OUT"
    return None

def side_to_action(val: str) -> str | None:
    if val is None or (isinstance(val, float) and np.isnan(val)):
        return None
    s = str(val).replace("ã€€","").replace(" ","").lower()
    if "è²·å»º" in s: return "è²·å»º"
    if "å£²å»º" in s: return "å£²å»º"
    if ("å£²åŸ‹" in s) or ("å£²è¿”æ¸ˆ" in s): return "å£²åŸ‹"
    if ("è²·åŸ‹" in s) or ("è²·è¿”æ¸ˆ" in s): return "è²·åŸ‹"
    if "buy" in s and "close" not in s:  return "è²·å»º"
    if "sell" in s and "close" not in s: return "å£²å»º"
    if "sell" in s and "close" in s:     return "å£²åŸ‹"
    if "buy" in s and "close" in s:      return "è²·åŸ‹"
    return None

# ---- ç´„å®šï¼ˆç´„å®šå±¥æ­´ï¼‰â†’ IN/OUT ãƒãƒ¼ã‚«ãƒ¼ç”Ÿæˆã®è£œåŠ©
def build_exec_table_allperiod(yj_all: pd.DataFrame) -> pd.DataFrame:
    if yj_all is None or yj_all.empty:
        return pd.DataFrame(columns=["code_key","name_key","exec_time","price","io","action"])
    d = yj_all.copy()

    dtcol = pick_dt_col(d); tmcol = pick_time_col(d)
    if dtcol is None:
        return pd.DataFrame(columns=["code_key","name_key","exec_time","price","io","action"])

    def has_time_rowwise(row):
        val_dt = str(row.get(dtcol, "")).strip()
        val_tm = str(row.get(tmcol, "")).strip() if tmcol else ""
        if val_tm:
            return True
        if re.search(r"[:ï¼š]", val_dt): return True
        if re.search(r"\b\d{3,6}\b", val_dt): return True
        if re.search(r"æ™‚\d{1,2}åˆ†", val_dt): return True
        return False

    mask_time = d.apply(has_time_rowwise, axis=1)
    d = d.loc[mask_time].copy()
    if d.empty:
        return pd.DataFrame(columns=["code_key","name_key","exec_time","price","io","action"])

    if tmcol:
        exec_ts = combine_date_time_cols(d, dtcol, tmcol)
    else:
        exec_ts = parse_datetime_from_dtcol(d, dtcol)
    d["exec_time"] = exec_ts

    # ä¾¡æ ¼
    PRICE_CANDS = ["ç´„å®šå˜ä¾¡(å††)","ç´„å®šå˜ä¾¡ï¼ˆå††ï¼‰","ç´„å®šä¾¡æ ¼","ä¾¡æ ¼","ç´„å®šå˜ä¾¡"]
    def select_price_series(df: pd.DataFrame) -> pd.Series | None:
        for c in PRICE_CANDS:
            if c in df.columns: return to_numeric_jp(df[c])
        pat = re.compile(r"(ç´„å®š)?.*(å˜ä¾¡|ä¾¡æ ¼)")
        best,nn = None,-1
        for c in df.columns:
            if pat.search(str(c)):
                s = to_numeric_jp(df[c]); k = s.notna().sum()
                if k>nn: best,nn = c,k
        return to_numeric_jp(df[best]) if best else None

    price_series = select_price_series(d)
    d["price"] = price_series if price_series is not None else np.nan

    d = normalize_symbol_cols(d)

    side_col = None
    for c in ["å£²è²·","å£²è²·åŒºåˆ†","å£²è²·ç¨®åˆ¥","Side","å–å¼•"]:
        if c in d.columns: side_col = c; break
    if side_col is None:
        for c in d.columns:
            if "å£²è²·" in c or "side" in c.lower() or "å–å¼•" in c:
                side_col = c; break
    d["io"] = d[side_col].map(side_to_io) if side_col else None
    d["action"] = d[side_col].map(side_to_action) if side_col else None

    cols = ["code_key","name_key","exec_time","price","io","action"]
    out = d[cols].dropna(subset=["exec_time"]).copy()
    out["code_key"] = out["code_key"].astype("string").str.upper().str.strip().replace({"":"<NA>","NAN":"<NA>"}).replace("<NA>", pd.NA)
    return out

def build_trade_table_for_display(yakujyou_all: pd.DataFrame, sel_date: date, code4: str) -> pd.DataFrame:
    if yakujyou_all is None or yakujyou_all.empty:
        return pd.DataFrame(columns=["ç´„å®šæ™‚é–“","å£²è²·","ç´„å®šæ•°","ç´„å®šå˜ä¾¡"])

    d = normalize_symbol_cols(yakujyou_all.copy())
    dtcol = pick_dt_col(d)
    tmcol = pick_time_col(d)
    if dtcol is None:
        return pd.DataFrame(columns=["ç´„å®šæ™‚é–“","å£²è²·","ç´„å®šæ•°","ç´„å®šå˜ä¾¡"])

    if tmcol:
        ts = combine_date_time_cols(d, dtcol, tmcol)
    else:
        ts = parse_datetime_from_dtcol(d, dtcol)
    d["__exec_time__"] = ts

    d["code_key"] = d["code_key"].astype("string").str.upper().str.strip()
    mask = (d["code_key"] == str(code4).upper()) & (d["__exec_time__"].dt.tz_convert(TZ).dt.date == sel_date)
    d = d.loc[mask].copy()
    if d.empty:
        return pd.DataFrame(columns=["ç´„å®šæ™‚é–“","å£²è²·","ç´„å®šæ•°","ç´„å®šå˜ä¾¡"])

    side_col = None
    for c in ["å£²è²·","å£²è²·åŒºåˆ†","å£²è²·ç¨®åˆ¥","Side","å–å¼•"]:
        if c in d.columns: side_col = c; break
    if side_col is None:
        for c in d.columns:
            if "å£²è²·" in c or "side" in c.lower() or "å–å¼•" in c:
                side_col = c; break

    def _side_to_action(val: str) -> str | None:
        if val is None or (isinstance(val, float) and np.isnan(val)): return None
        s = str(val).replace("ã€€","").replace(" ","").lower()
        if "è²·å»º" in s: return "è²·å»º"
        if "å£²å»º" in s: return "å£²å»º"
        if ("å£²åŸ‹" in s) or ("å£²è¿”æ¸ˆ" in s): return "å£²åŸ‹"
        if ("è²·åŸ‹" in s) or ("è²·è¿”æ¸ˆ" in s): return "è²·åŸ‹"
        if "buy" in s and "close" not in s:  return "è²·å»º"
        if "sell" in s and "close" not in s: return "å£²å»º"
        if "sell" in s and "close" in s:     return "å£²åŸ‹"
        if "buy" in s and "close" in s:      return "è²·åŸ‹"
        return None

    if side_col:
        side_series = d[side_col].astype(str)
        action_series = d[side_col].map(_side_to_action)
        side_series = side_series.where(side_series.str.strip().ne(""), action_series)
    else:
        side_series = d.get("action", None)

    def select_qty_series(df: pd.DataFrame) -> pd.Series | None:
        cand_exact = [
            "ç´„å®šæ•°é‡", "ç´„å®šæ•°é‡(æ ª)", "ç´„å®šæ•°é‡ï¼ˆæ ªï¼‰", "ç´„å®šæ ªæ•°",
            "å‡ºæ¥æ•°é‡", "æ•°é‡", "æ ªæ•°", "å‡ºæ¥é«˜",
            "ç´„å®šæ•°é‡(å£)", "å£æ•°"
        ]
        for c in cand_exact:
            if c in df.columns:
                return to_numeric_jp(df[c])
        best, nn = None, -1
        for c in df.columns:
            cname = str(c).replace(" ", "")
            if any(k in cname for k in ["æ•°é‡", "æ ªæ•°", "å£æ•°", "å‡ºæ¥é«˜"]):
                s = to_numeric_jp(df[c])
                k = s.notna().sum()
                if k > nn:
                    best, nn = c, k
        return to_numeric_jp(df[best]) if best else None

    def select_price_series(df: pd.DataFrame) -> pd.Series | None:
        PRICE_CANDS = ["ç´„å®šå˜ä¾¡(å††)","ç´„å®šå˜ä¾¡ï¼ˆå††ï¼‰","ç´„å®šä¾¡æ ¼","ä¾¡æ ¼","ç´„å®šå˜ä¾¡"]
        for c in PRICE_CANDS:
            if c in df.columns: return to_numeric_jp(df[c])
        pat = re.compile(r"(ç´„å®š)?.*(å˜ä¾¡|ä¾¡æ ¼)")
        best,nn = None,-1
        for c in df.columns:
            if pat.search(str(c)):
                s = to_numeric_jp(df[c]); k = s.notna().sum()
                if k>nn: best,nn = c,k
        return to_numeric_jp(df[best]) if best else None

    qty_series   = select_qty_series(d)
    price_series = select_price_series(d)

    out = pd.DataFrame({
        "ç´„å®šæ™‚é–“": d["__exec_time__"].dt.tz_convert(TZ).dt.strftime("%H:%M:%S"),
        "å£²è²·": side_series,
        "ç´„å®šæ•°": qty_series,
        "ç´„å®šå˜ä¾¡": price_series,
    })
    out = out.sort_values("ç´„å®šæ™‚é–“").reset_index(drop=True)
    return out

# =========================================================
# 3åˆ†è¶³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç¾¤ã‹ã‚‰ä½œã‚‹ï¼‰
# =========================================================
@st.cache_data(show_spinner=False)
def load_ohlc_map_from_uploads(files, sig: str):
    """
    time, open, high, low, close ã‚’å¿…é ˆã¨ã—ã€ä»»æ„ã§ VWAP, MA1, MA2, MA3 ã‚’å–ã‚Šè¾¼ã‚€ã€‚
    ã‚­ãƒ¼ã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã®ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­é™¤ãï¼‰ã€‚
    """
    ohlc_map = {}
    if not files: return ohlc_map
    for f in files:
        df = read_table_from_upload(f.name, f.getvalue())
        if df.empty: continue

        col_rename_map, found_cols = {}, {}
        CANDIDATES = {
            "time": ["time", "æ—¥æ™‚"],
            "open": ["open", "å§‹å€¤"],
            "high": ["high", "é«˜å€¤"],
            "low":  ["low",  "å®‰å€¤"],
            "close":["close","çµ‚å€¤"],
        }
        original_cols = {c.lower(): c for c in df.columns}
        for std, cands in CANDIDATES.items():
            for cand in cands:
                if cand in original_cols:
                    col_rename_map[original_cols[cand]] = std
                    found_cols[std] = True
                    break
        if len(found_cols) < len(CANDIDATES):
            continue

        df = df.rename(columns=col_rename_map)

        t = pd.to_datetime(df["time"], errors="coerce", infer_datetime_format=True)
        if getattr(t.dtype, "tz", None) is None: t = t.dt.tz_localize(TZ)
        else: t = t.dt.tz_convert(TZ)
        df = df.copy()
        df["time"] = t

        def pick_one(df, names):
            for n in names:
                if n in df.columns: return df[n]
            return None
        vwap = pick_one(df, ["VWAP","vwap","Vwap"])
        ma1  = pick_one(df, ["MA1","ma1","Ma1"])
        ma2  = pick_one(df, ["MA2","ma2","Ma2"])
        ma3  = pick_one(df, ["MA3","ma3","Ma3"])
        if vwap is not None: df["VWAP"] = pd.to_numeric(vwap, errors="coerce")
        if ma1  is not None: df["MA1"]  = pd.to_numeric(ma1,  errors="coerce")
        if ma2  is not None: df["MA2"]  = pd.to_numeric(ma2,  errors="coerce")
        if ma3  is not None: df["MA3"]  = pd.to_numeric(ma3,  errors="coerce")

        df = df.dropna(subset=["time"]).sort_values("time")
        df = df.drop_duplicates(subset=["time"], keep="last").reset_index(drop=True)

        key = f.name.rsplit(".",1)[0]
        ohlc_map[key] = df
    return ohlc_map

def extract_code_from_ohlc_key(key: str):
    m = re.search(r'_(?i:(\d{3,5}[a-z]))(?=[,_])', key)
    if m: return m.group(1).upper()
    m2 = re.search(r'_(\d{4,5})(?=[,_])', key)
    if m2: return m2.group(1)
    return None

def build_ohlc_code_index(ohlc_map: dict):
    idx = {}
    for k in ohlc_map.keys():
        c = extract_code_from_ohlc_key(k)
        if c:
            idx.setdefault(c.upper(), []).append(k)
    return idx

def ohlc_global_date_range(ohlc_map: dict):
    if not ohlc_map: return None, None
    mins, maxs = [], []
    for df in ohlc_map.values():
        if df is None or df.empty or "time" not in df.columns: continue
        t = df["time"]
        t = t.dt.tz_localize(TZ) if getattr(t.dtype,"tz",None) is None else t.dt.tz_convert(TZ)
        if t.notna().any():
            mins.append(t.min().date()); maxs.append(t.max().date())
    if not mins or not maxs: return None, None
    return min(mins), max(maxs)

def market_time(sel_date: date, start_str="09:00", end_str="15:30"):
    return pd.Timestamp(f"{sel_date} {start_str}", tz=TZ), pd.Timestamp(f"{sel_date} {end_str}", tz=TZ)

def pick_best_ohlc_key_for_date(code4: str, ohlc_code_index: dict, ohlc_map: dict, sel_date) -> tuple[str|None, pd.DataFrame|None, str]:
    keys = ohlc_code_index.get(str(code4).upper(), [])
    if not keys: return None, None, "è©²å½“ã‚­ãƒ¼ãªã—"
    start_dt, end_dt = market_time(sel_date)
    candidates, dbg = [], []
    for k in keys:
        df = ohlc_map.get(k)
        if df is None or df.empty or "time" not in df.columns:
            dbg.append(f"{k}: ãƒ‡ãƒ¼ã‚¿ãªã—/åˆ—ä¸å‚™"); continue
        t = df["time"]
        t = t.dt.tz_localize(TZ) if getattr(t.dtype,"tz",None) is None else t.dt.tz_convert(TZ)
        tmin, tmax = t.min(), t.max()
        dbg.append(f"{k}: [{tmin} .. {tmax}]")
        in_win = (t>=start_dt)&(t<=end_dt)
        same_day = (t.dt.date==sel_date)
        day_dist = float("inf")
        if t.notna().any():
            sel_start = pd.Timestamp(sel_date, tz=TZ)
            if (t.dt.date==sel_date).any(): day_dist = 0.0
            elif tmax < sel_start: day_dist = (sel_start - tmax).total_seconds()
            elif tmin > sel_start: day_dist = (tmin - sel_start).total_seconds()
            else: day_dist = 0.0
        score = (0 if in_win.any() else (1 if same_day.any() else 2), 0 if same_day.any() else 1, day_dist)
        candidates.append((score,k,df))
    if not candidates: return None, None, "å€™è£œãªã—"
    candidates.sort(key=lambda x:x[0])
    best = candidates[0]
    return best[1], best[2], " / ".join(dbg)+f" => PICK: {best[1]}"

def download_button_df(df, label, filename):
    csv = df.to_csv(index=False).encode("utf-8-sig")
    st.download_button(label=label, data=csv, file_name=filename, mime="text/csv")

def compute_max_drawdown(series: pd.Series) -> float:
    if series is None:
        return np.nan
    s = pd.to_numeric(series, errors="coerce").dropna()
    if s.empty:
        return np.nan
    arr = s.to_numpy(dtype=float)
    peak, max_dd = -np.inf, 0.0
    for x in arr:
        peak = max(peak, x)
        max_dd = max(max_dd, peak - x)
    return float(max_dd)

# ======== ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ æ¤œå‡ºï¼†ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ========
def _detect_base_interval_minutes(ts: pd.Series) -> int | None:
    if ts is None or ts.empty: return None
    t = pd.to_datetime(ts, errors="coerce")
    if getattr(t.dtype, "tz", None) is None:
        t = t.dt.tz_localize(TZ)
    diffs = t.sort_values().diff().dropna().dt.total_seconds() / 60
    if diffs.empty: return None
    return int(round(diffs.median()))

def _resample_ohlc(df: pd.DataFrame, rule: str) -> pd.DataFrame:
    need_cols = {"time","open","high","low","close"}
    if df.empty or not need_cols.issubset(df.columns): 
        return df.copy()

    d = df.copy()
    t = pd.to_datetime(d["time"], errors="coerce")
    if getattr(t.dtype, "tz", None) is None:
        t = t.dt.tz_localize(TZ)
    d["time"] = t
    d = d.set_index("time")

    agg = {"open":"first","high":"max","low":"min","close":"last"}
    if "volume" in d.columns:
        agg["volume"] = "sum"

    out = d.resample(rule, origin="start_day").agg(agg).dropna(subset=["open","high","low","close"])

    if "VWAP" in d.columns:
        if "volume" in d.columns:
            wnum = (d["VWAP"] * d["volume"]).resample(rule, origin="start_day").sum(min_count=1)
            wden = d["volume"].resample(rule, origin="start_day").sum(min_count=1)
            out["VWAP"] = (wnum / wden)
        else:
            out["VWAP"] = d["VWAP"].resample(rule, origin="start_day").mean()

    for ma in ["MA1","MA2","MA3"]:
        if ma in d.columns:
            out[ma] = pd.to_numeric(d[ma], errors="coerce").resample(rule, origin="start_day").mean()

    out = out.reset_index()
    return out

# ======== æ™‚é–“ç³»ãƒ˜ãƒ«ãƒ‘ï¼ˆâ€œç§’â€ã§æ¯”è¼ƒã™ã‚‹æ–¹å¼ã«çµ±ä¸€ï¼‰ ========
BASE_ANCHOR_DATE = datetime(2000,1,1, tzinfo=TZ)

def to_time_of_day_ts(dt_series: pd.Series) -> pd.Series:
    dt_local = dt_series.dt.tz_convert(TZ)
    return pd.to_datetime([datetime(2000,1,1,h, m, s, tzinfo=TZ)
                           for h,m,s in zip(dt_local.dt.hour, dt_local.dt.minute, dt_local.dt.second)])

def session_of(dt_series: pd.Series) -> pd.Series:
    """å‰å ´/å¾Œå ´/ãã®ä»–ï¼ˆNaï¼‰ã‚’åˆ¤å®šï¼ˆç§’ã§æ¯”è¼ƒï¼‰"""
    dt_local = dt_series.dt.tz_convert(TZ)
    sec = dt_local.dt.hour*3600 + dt_local.dt.minute*60 + dt_local.dt.second
    out = pd.Series(pd.NA, index=dt_series.index, dtype="object")
    out[(sec >= MORNING_START_SEC) & (sec <= MORNING_END_SEC)]  = "å‰å ´"
    out[(sec >= AFTERNOON_START_SEC) & (sec <= AFTERNOON_END_SEC)] = "å¾Œå ´"
    return out

def clip_market_time(dt_series: pd.Series) -> pd.Series:
    """9:00ï½15:30 ã«å…¥ã£ã¦ã„ã‚‹è¡Œã ã‘ Trueï¼ˆç§’ã§æ¯”è¼ƒï¼‰"""
    dt_local = dt_series.dt.tz_convert(TZ)
    sec = dt_local.dt.hour*3600 + dt_local.dt.minute*60 + dt_local.dt.second
    return (sec >= MORNING_START_SEC) & (sec <= AFTERNOON_END_SEC)

# =========================================================
# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ & ãƒ•ã‚£ãƒ«ã‚¿
# =========================================================
st.sidebar.header("â‘  ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã¾ã¨ã‚ã¦OKï¼‰")
realized_files = st.sidebar.file_uploader("å®Ÿç¾æç›Š CSV/Excel", type=["csv","txt","xlsx"], accept_multiple_files=True)
yakujyou_files = st.sidebar.file_uploader("ç´„å®šå±¥æ­´ CSV/Excel", type=["csv","txt","xlsx"], accept_multiple_files=True)
ohlc_files     = st.sidebar.file_uploader("3åˆ†è¶³ OHLC CSV/Excelï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åæœ«å°¾ã« _7974 ãªã©ã®ã‚³ãƒ¼ãƒ‰ã‚’å«ã‚ã‚‹ï¼‰", type=["csv","txt","xlsx"], accept_multiple_files=True)

sig_realized = files_signature(realized_files)
sig_yakujyou = files_signature(yakujyou_files)
sig_ohlc     = files_signature(ohlc_files)

yakujyou_all = concat_uploaded_tables(yakujyou_files, sig_yakujyou)
yakujyou_all = normalize_yakujyou(clean_columns(yakujyou_all))

realized = concat_uploaded_tables(realized_files, sig_realized)
realized = normalize_realized(clean_columns(realized))

ohlc_map = load_ohlc_map_from_uploads(ohlc_files, sig_ohlc)
CODE_TO_NAME = build_code_to_name_map(realized, yakujyou_all)

# ä¿¡ç”¨åŒºåˆ†ãƒ•ã‚£ãƒ«ã‚¿
st.sidebar.header("â‘¡ ãƒˆãƒ¬ãƒ¼ãƒ‰ç¨®åˆ¥ãƒ•ã‚£ãƒ«ã‚¿")
trade_type = st.sidebar.radio("å¯¾è±¡", ["å…¨ä½“","ãƒ‡ã‚¤ãƒˆãƒ¬ãƒ¼ãƒ‰ï¼ˆä¸€èˆ¬ï¼‰","ã‚¹ã‚¤ãƒ³ã‚°ãƒˆãƒ¬ãƒ¼ãƒ‰ï¼ˆåˆ¶åº¦ï¼‰"], index=0)

def apply_trade_type_filter(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty or "ä¿¡ç”¨åŒºåˆ†" not in df.columns: return df
    if trade_type.startswith("å…¨ä½“"): return df
    if "ãƒ‡ã‚¤ãƒˆãƒ¬" in trade_type: return df[df["ä¿¡ç”¨åŒºåˆ†"]=="ä¸€èˆ¬"]
    if "ã‚¹ã‚¤ãƒ³ã‚°" in trade_type: return df[df["ä¿¡ç”¨åŒºåˆ†"]=="åˆ¶åº¦"]
    return df

# æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿
st.subheader("æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿")
c1,c2,c3 = st.columns([2,2,3])
with c1:
    span = st.radio("ã‚¯ã‚¤ãƒƒã‚¯é¸æŠ", ["å…¨æœŸé–“","ä»Šæ—¥","ä»Šé€±","ä»Šæœˆ","ä»Šå¹´","ã‚«ã‚¹ã‚¿ãƒ "], horizontal=True, index=0, key="span")
with c2:
    start = st.date_input("é–‹å§‹æ—¥", value=date.today()-timedelta(days=30), disabled=(span!="ã‚«ã‚¹ã‚¿ãƒ "))
with c3:
    end = st.date_input("çµ‚äº†æ—¥", value=date.today(), disabled=(span!="ã‚«ã‚¹ã‚¿ãƒ "))

def filter_by_span(df, dt_col):
    if df.empty or dt_col not in df.columns: return df
    dt = pd.to_datetime(df[dt_col], errors="coerce")
    today = date.today()
    if span=="å…¨æœŸé–“": return df
    if span=="ä»Šæ—¥": s,e = today, today
    elif span=="ä»Šé€±":
        mon = today - timedelta(days=today.weekday()); s,e = mon, mon+timedelta(days=6)
    elif span=="ä»Šæœˆ":
        first = today.replace(day=1)
        next_f = date(first.year+1,1,1) if first.month==12 else date(first.year, first.month+1, 1)
        s,e = first, next_f - timedelta(days=1)
    elif span=="ä»Šå¹´":
        s,e = date(today.year,1,1), date(today.year,12,31)
    else:
        s,e = start, end
    mask = (dt.dt.date>=s)&(dt.dt.date<=e)
    return df.loc[mask]

realized_f = apply_trade_type_filter(filter_by_span(realized, "ç´„å®šæ—¥"))

# =========================================================
# KPI
# =========================================================
st.subheader("KPI")

if not realized_f.empty and "å®Ÿç¾æç›Š[å††]" in realized_f.columns:
    realized_f["å®Ÿç¾æç›Š[å††]"] = to_numeric_jp(realized_f["å®Ÿç¾æç›Š[å††]"])

pl = None
if not realized_f.empty and "å®Ÿç¾æç›Š[å††]" in realized_f.columns:
    pl = to_numeric_jp(realized_f["å®Ÿç¾æç›Š[å††]"]).dropna()

c1,c2,c3 = st.columns(3)
with c1:
    total_pl = pl.sum() if pl is not None and not pl.empty else np.nan
    st.metric("å®Ÿç¾æç›Šï¼ˆé¸æŠæœŸé–“ï¼‰", f"{int(total_pl):,} å††" if pd.notna(total_pl) else "â€”")
with c2:
    n_trades = int(pl.shape[0]) if pl is not None else 0
    st.metric("å–å¼•å›æ•°", f"{n_trades:,}" if n_trades else "â€”")
with c3:
    avg_pl = pl.mean() if pl is not None and not pl.empty else np.nan
    st.metric("å¹³å‡æç›Šï¼ˆ/å›ï¼‰", f"{int(round(avg_pl)):,} å††" if pd.notna(avg_pl) else "â€”")

c4,c5,c6 = st.columns(3)
with c4:
    win_rate = (pl>0).mean()*100 if pl is not None and not pl.empty else np.nan
    st.metric("å‹ç‡ï¼ˆå…¨ä½“ï¼‰", f"{win_rate:.1f}%" if pd.notna(win_rate) else "â€”")

wr_long = wr_short = np.nan
if not realized_f.empty and ("å–å¼•" in realized_f.columns):
    rj = realized_f.copy()
    rj["PL"] = to_numeric_jp(rj["å®Ÿç¾æç›Š[å††]"])
    rj["LR"] = rj["å–å¼•"].map(lr_from_realized_trade)
    pl_long  = rj.loc[rj["LR"]=="LONG","PL"]
    pl_short = rj.loc[rj["LR"]=="SHORT","PL"]
    wr_long  = (pl_long>0).mean()*100  if pl_long.notna().any()  else np.nan
    wr_short = (pl_short>0).mean()*100 if pl_short.notna().any() else np.nan

with c5: st.metric("å‹ç‡ï¼ˆãƒ­ãƒ³ã‚°ï¼‰", f"{wr_long:.1f}%" if pd.notna(wr_long) else "â€”")
with c6: st.metric("å‹ç‡ï¼ˆã‚·ãƒ§ãƒ¼ãƒˆï¼‰", f"{wr_short:.1f}%" if pd.notna(wr_short) else "â€”")

# æœ€å¤§DD
c7,_c8,_c9 = st.columns(3)
with c7:
    if not realized_f.empty and "ç´„å®šæ—¥" in realized_f.columns and "å®Ÿç¾æç›Š[å††]" in realized_f.columns:
        tmp = realized_f.copy()
        tmp["æ—¥"] = pd.to_datetime(tmp["ç´„å®šæ—¥"], errors="coerce").dt.date
        tmp["å®Ÿç¾æç›Š[å††]"] = to_numeric_jp(tmp["å®Ÿç¾æç›Š[å††]"])
        seq = tmp.groupby("æ—¥", as_index=False)["å®Ÿç¾æç›Š[å††]"].sum().sort_values("æ—¥")
        seq["ç´¯è¨ˆ"] = pd.to_numeric(seq["å®Ÿç¾æç›Š[å††]"], errors="coerce").cumsum()
        dd = compute_max_drawdown(seq["ç´¯è¨ˆ"])
        st.metric("æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³", f"{int(round(dd)):,} å††" if pd.notna(dd) else "â€”")
    else:
        st.metric("æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³", "â€”")

# =========================================================
# ã‚¿ãƒ–
# =========================================================
tab1, tab1b, tab2, tab3, tab4, tab5 = st.tabs([
    "é›†è¨ˆï¼ˆæœŸé–“åˆ¥ï¼‰",
    "é›†è¨ˆï¼ˆæ™‚é–“åˆ¥ï¼‰",
    "ç´¯è¨ˆæç›Š",
    "å€‹åˆ¥éŠ˜æŸ„",
    "ãƒ©ãƒ³ã‚­ãƒ³ã‚°",
    "3åˆ†è¶³ IN/OUT + æŒ‡æ¨™"
])

# ---- 1) æœŸé–“åˆ¥
with tab1:
    st.markdown("### å®Ÿç¾æç›Šï¼ˆæœŸé–“åˆ¥é›†è¨ˆï¼‰")
    if realized_f.empty:
        st.info("å®Ÿç¾æç›Šãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã€‚")
    else:
        r = realized_f.copy()
        dts = pd.to_datetime(r["ç´„å®šæ—¥"], errors="coerce")
        r["å®Ÿç¾æç›Š[å††]"] = to_numeric_jp(r["å®Ÿç¾æç›Š[å††]"])
        r["æ—¥"] = dts.dt.date
        r["é€±"] = (dts - pd.to_timedelta(dts.dt.weekday, unit="D")).dt.date
        r["æœˆ"] = dts.dt.to_period("M").dt.to_timestamp().dt.date
        r["å¹´"] = dts.dt.to_period("Y").dt.to_timestamp().dt.date
        for label,col in [("æ—¥åˆ¥","æ—¥"),("é€±åˆ¥","é€±"),("æœˆåˆ¥","æœˆ"),("å¹´åˆ¥","å¹´")]:
            g = r.groupby(col, as_index=False)["å®Ÿç¾æç›Š[å††]"].sum().sort_values(col)
            st.write(f"**{label}**")
            st.dataframe(g, use_container_width=True, hide_index=True)
            download_button_df(g, f"â¬‡ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆ{label}ï¼‰", f"{col}_pl.csv")
            fig_bar = go.Figure([go.Bar(x=g[col], y=g["å®Ÿç¾æç›Š[å††]"], name=f"{label} å®Ÿç¾æç›Š")])
            fig_bar.update_layout(margin=dict(l=10,r=10,t=20,b=10), height=300, xaxis_title=label, yaxis_title="å®Ÿç¾æç›Š[å††]")
            st.plotly_chart(fig_bar, use_container_width=True)

# ---- 1b) æ™‚é–“åˆ¥
with tab1b:
    st.markdown("### å®Ÿç¾æç›Šï¼ˆæ™‚é–“åˆ¥ãƒ»1æ™‚é–“ã”ã¨ï¼‰")
    if realized_f.empty:
        st.info("å®Ÿç¾æç›Šãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã€‚")
    else:
        d = realized_f.copy()

        # --- æ™‚åˆ»ãŒã‚ã‚‹è¡Œã ã‘æ¡ç”¨ ---
        if "ç´„å®šæ—¥æ™‚" not in d.columns:
            st.warning("å®Ÿç¾æç›Šãƒ‡ãƒ¼ã‚¿ã« 'ç´„å®šæ—¥æ™‚' åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚„åˆ—æ¤œå‡ºã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
        else:
            dt = pd.to_datetime(d["ç´„å®šæ—¥æ™‚"], errors="coerce")
            try:
                dt = dt.dt.tz_convert(TZ)
            except Exception:
                dt = dt.dt.tz_localize(TZ)

            if "ç´„å®šæ™‚åˆ»ã‚ã‚Š" in d.columns:
                d = d.loc[d["ç´„å®šæ™‚åˆ»ã‚ã‚Š"]].copy()
                dt = dt.loc[d.index]
            else:
                mask_has_time = dt.notna() & ((dt.dt.hour + dt.dt.minute + dt.dt.second) > 0)
                d = d.loc[mask_has_time].copy()
                dt = dt.loc[d.index]

            # å¸‚å ´æ™‚é–“ã§ã‚¯ãƒªãƒƒãƒ—ï¼ˆ9:00ã€œ15:30ï¼‰â€” ç§’ã§æ¯”è¼ƒ
            dt_local = dt.dt.tz_convert(TZ)
            sec = dt_local.dt.hour*3600 + dt_local.dt.minute*60 + dt_local.dt.second
            mask_mkt = (sec >= MORNING_START_SEC) & (sec <= AFTERNOON_END_SEC)
            d = d.loc[mask_mkt].copy()
            dt = dt.loc[d.index]

            if d.empty:
                st.info("æ™‚åˆ»ä»˜ãã®ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆã¾ãŸã¯å¸‚å ´æ™‚é–“å¤–ã®ã¿ã§ã—ãŸï¼‰ã€‚")
            else:
                # æŒ‡æ¨™åˆ—
                d["PL"] = to_numeric_jp(d["å®Ÿç¾æç›Š[å††]"])
                d["win"] = d["PL"] > 0
                d["LR"]  = d["å–å¼•"].map(lr_from_realized_trade) if "å–å¼•" in d.columns else pd.NA

                # 1æ™‚é–“ãƒ“ãƒ³ï¼ˆxè»¸ã¯å›ºå®š9:00ã€œ15:30ã®æ“¬ä¼¼æ—¥ä»˜ 2000-01-01 ã‚’ä½¿ç”¨ï¼‰
                hour_floor = dt.dt.floor("H")
                hour_x = pd.to_datetime([datetime(2000,1,1,h.hour,0,0, tzinfo=TZ) for h in hour_floor])
                d["hour_x"] = hour_x

                x_range = [datetime(2000,1,1,9,0, tzinfo=TZ), datetime(2000,1,1,15,30, tzinfo=TZ)]
                x_ticks = pd.date_range(x_range[0], x_range[1], freq="60min", inclusive="both")
                base = pd.DataFrame({"hour_x": x_ticks})

                by = d.groupby("hour_x", as_index=False).agg(
                    åæ”¯=("PL","sum"),
                    å–å¼•å›æ•°=("PL","count"),
                    å‹ç‡=("win","mean"),
                    å¹³å‡æç›Š=("PL","mean")
                )
                if d["LR"].notna().any():
                    gL = d[d["LR"]=="LONG"].groupby("hour_x")["win"].mean().rename("å‹ç‡_ãƒ­ãƒ³ã‚°")
                    gS = d[d["LR"]=="SHORT"].groupby("hour_x")["win"].mean().rename("å‹ç‡_ã‚·ãƒ§ãƒ¼ãƒˆ")
                    by = base.merge(by, on="hour_x", how="left").merge(gL, how="left", on="hour_x").merge(gS, how="left", on="hour_x")
                else:
                    by = base.merge(by, on="hour_x", how="left")
                    by["å‹ç‡_ãƒ­ãƒ³ã‚°"] = np.nan
                    by["å‹ç‡_ã‚·ãƒ§ãƒ¼ãƒˆ"] = np.nan

                disp = by.copy()
                disp["æ™‚é–“"] = disp["hour_x"].dt.strftime("%H:%M")
                disp["å‹ç‡"] = (disp["å‹ç‡"]*100).round(1)
                disp["å‹ç‡_ãƒ­ãƒ³ã‚°"] = (disp["å‹ç‡_ãƒ­ãƒ³ã‚°"]*100).round(1)
                disp["å‹ç‡_ã‚·ãƒ§ãƒ¼ãƒˆ"] = (disp["å‹ç‡_ã‚·ãƒ§ãƒ¼ãƒˆ"]*100).round(1)
                st.dataframe(disp[["æ™‚é–“","åæ”¯","å–å¼•å›æ•°","å‹ç‡","å‹ç‡_ãƒ­ãƒ³ã‚°","å‹ç‡_ã‚·ãƒ§ãƒ¼ãƒˆ","å¹³å‡æç›Š"]],
                             use_container_width=True, hide_index=True)
                download_button_df(disp, "â¬‡ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæ™‚é–“åˆ¥ï¼‰", "hourly_stats.csv")

                # ã‚°ãƒ©ãƒ•
                fig_h_pl = go.Figure([go.Bar(x=by["hour_x"], y=by["åæ”¯"], name="åæ”¯ï¼ˆåˆè¨ˆï¼‰")])
                fig_h_pl.update_layout(title="æ™‚é–“åˆ¥ åæ”¯ï¼ˆåˆè¨ˆï¼‰", xaxis_title="æ™‚é–“", yaxis_title="å††",
                                       margin=dict(l=10,r=10,t=30,b=10), height=300,
                                       xaxis=dict(tickformat="%H:%M", range=x_range))
                st.plotly_chart(fig_h_pl, use_container_width=True)

                fig_h_wr = go.Figure([go.Scatter(x=by["hour_x"], y=by["å‹ç‡"]*100, mode="lines+markers", name="å‹ç‡")])
                fig_h_wr.update_layout(title="æ™‚é–“åˆ¥ å‹ç‡ï¼ˆå…¨ä½“ï¼‰", xaxis_title="æ™‚é–“", yaxis_title="å‹ç‡ï¼ˆ%ï¼‰",
                                       margin=dict(l=10,r=10,t=30,b=10), height=300,
                                       yaxis=dict(range=[0,100]),
                                       xaxis=dict(tickformat="%H:%M", range=x_range))
                st.plotly_chart(fig_h_wr, use_container_width=True)

                fig_h_lr = go.Figure()
                fig_h_lr.add_trace(go.Scatter(x=by["hour_x"], y=by["å‹ç‡_ãƒ­ãƒ³ã‚°"]*100, mode="lines+markers", name="å‹ç‡ï¼ˆãƒ­ãƒ³ã‚°ï¼‰"))
                fig_h_lr.add_trace(go.Scatter(x=by["hour_x"], y=by["å‹ç‡_ã‚·ãƒ§ãƒ¼ãƒˆ"]*100, mode="lines+markers", name="å‹ç‡ï¼ˆã‚·ãƒ§ãƒ¼ãƒˆï¼‰"))
                fig_h_lr.update_layout(title="æ™‚é–“åˆ¥ å‹ç‡ï¼ˆãƒ­ãƒ³ã‚°/ã‚·ãƒ§ãƒ¼ãƒˆï¼‰", xaxis_title="æ™‚é–“", yaxis_title="å‹ç‡ï¼ˆ%ï¼‰",
                                       margin=dict(l=10,r=10,t=30,b=10), height=300,
                                       yaxis=dict(range=[0,100]),
                                       xaxis=dict(tickformat="%H:%M", range=x_range))
                st.plotly_chart(fig_h_lr, use_container_width=True)

                fig_h_cnt = go.Figure([go.Bar(x=by["hour_x"], y=by["å–å¼•å›æ•°"], name="å–å¼•å›æ•°")])
                fig_h_cnt.update_layout(title="æ™‚é–“åˆ¥ å–å¼•å›æ•°", xaxis_title="æ™‚é–“", yaxis_title="å›",
                                        margin=dict(l=10,r=10,t=30,b=10), height=300,
                                        xaxis=dict(tickformat="%H:%M", range=x_range))
                st.plotly_chart(fig_h_cnt, use_container_width=True)

                fig_h_avg = go.Figure([go.Bar(x=by["hour_x"], y=by["å¹³å‡æç›Š"], name="å¹³å‡æç›Šï¼ˆ/å›ï¼‰")])
                fig_h_avg.update_layout(title="æ™‚é–“åˆ¥ å¹³å‡æç›Šï¼ˆ/å›ï¼‰", xaxis_title="æ™‚é–“", yaxis_title="å††/å›",
                                        margin=dict(l=10,r=10,t=30,b=10), height=300,
                                        xaxis=dict(tickformat="%H:%M", range=x_range))
                st.plotly_chart(fig_h_avg, use_container_width=True)

                # å‰å ´ / å¾Œå ´ æ¯”è¼ƒ
                st.markdown("### å‰å ´ / å¾Œå ´ æ¯”è¼ƒ")
                ses = session_of(dt.loc[d.index])
                d["ã‚»ãƒƒã‚·ãƒ§ãƒ³"] = ses
                cmp = d.dropna(subset=["ã‚»ãƒƒã‚·ãƒ§ãƒ³"]).groupby("ã‚»ãƒƒã‚·ãƒ§ãƒ³").agg(
                    åæ”¯=("PL","sum"),
                    å–å¼•å›æ•°=("PL","count"),
                    å‹ç‡=("win","mean"),
                    å¹³å‡æç›Š=("PL","mean")
                ).reset_index()
                cmp["å‹ç‡"] = (cmp["å‹ç‡"]*100).round(1)
                st.dataframe(cmp, use_container_width=True, hide_index=True)
                download_button_df(cmp, "â¬‡ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå‰å ´å¾Œå ´æ¯”è¼ƒï¼‰", "am_pm_compare.csv")

                cc1, cc2 = st.columns(2)
                with cc1:
                    fig_cmp_pl = go.Figure([go.Bar(x=cmp["ã‚»ãƒƒã‚·ãƒ§ãƒ³"], y=cmp["åæ”¯"], name="åæ”¯")])
                    fig_cmp_pl.update_layout(title="å‰å ´/å¾Œå ´ åæ”¯", xaxis_title="", yaxis_title="å††",
                                             margin=dict(l=10,r=10,t=30,b=10), height=300)
                    st.plotly_chart(fig_cmp_pl, use_container_width=True)
                with cc2:
                    fig_cmp_wr = go.Figure([go.Bar(x=cmp["ã‚»ãƒƒã‚·ãƒ§ãƒ³"], y=cmp["å‹ç‡"], name="å‹ç‡")])
                    fig_cmp_wr.update_layout(title="å‰å ´/å¾Œå ´ å‹ç‡", xaxis_title="", yaxis_title="å‹ç‡ï¼ˆ%ï¼‰",
                                             margin=dict(l=10,r=10,t=30,b=10), height=300,
                                             yaxis=dict(range=[0,100]))
                    st.plotly_chart(fig_cmp_wr, use_container_width=True)

                # ç´¯ç©å‹ç‡ã®æ™‚é–“æ¨ç§»ï¼ˆ5åˆ†ãƒ“ãƒ³ï¼‰
                st.markdown("### ç´¯ç©å‹ç‡ã®æ™‚é–“æ¨ç§»ï¼ˆå…¨æœŸé–“ãƒ»5åˆ†ãƒ“ãƒ³ï¼‰")
                five_bin = dt.loc[d.index].dt.floor("5min")
                x_five = pd.to_datetime([datetime(2000,1,1,t.hour,t.minute,0, tzinfo=TZ) for t in five_bin.dt.time])
                tmp = pd.DataFrame({"x": x_five, "win": d["win"].astype(float), "cnt": 1.0})
                grid = pd.DataFrame({"x": pd.date_range(datetime(2000,1,1,9,0, tzinfo=TZ),
                                                        datetime(2000,1,1,15,30, tzinfo=TZ),
                                                        freq="5min", inclusive="both")})
                agg5 = tmp.groupby("x").agg(win_sum=("win","sum"), cnt=("cnt","sum")).reset_index()
                grid = grid.merge(agg5, on="x", how="left").fillna(0.0)
                grid["cum_wr"] = np.where(grid["cnt"].cumsum()>0,
                                          grid["win_sum"].cumsum()/grid["cnt"].cumsum()*100.0, np.nan)

                fig_cum = go.Figure([go.Scatter(x=grid["x"], y=grid["cum_wr"], mode="lines", name="ç´¯ç©å‹ç‡")])
                fig_cum.update_layout(title="ç´¯ç©å‹ç‡ï¼ˆæ™‚é–“ã®çµŒéã¨ã¨ã‚‚ã«ï¼‰", xaxis_title="æ™‚é–“", yaxis_title="å‹ç‡ï¼ˆ%ï¼‰",
                                      margin=dict(l=10,r=10,t=30,b=10), height=320,
                                      yaxis=dict(range=[0,100]),
                                      xaxis=dict(tickformat="%H:%M",
                                                 range=[datetime(2000,1,1,9,0, tzinfo=TZ),
                                                        datetime(2000,1,1,15,30, tzinfo=TZ)]))
                st.plotly_chart(fig_cum, use_container_width=True)

# ---- 2) ç´¯è¨ˆ
with tab2:
    st.markdown("### ç´¯è¨ˆå®Ÿç¾æç›Šï¼ˆé¸æŠæœŸé–“å†…ã€æ—¥æ¬¡ãƒ™ãƒ¼ã‚¹ï¼‰")
    if realized_f.empty:
        st.info("å®Ÿç¾æç›Šãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã€‚")
    else:
        d = realized_f.copy()
        d["æ—¥"] = pd.to_datetime(d["ç´„å®šæ—¥"]).dt.date
        d["å®Ÿç¾æç›Š[å††]"] = to_numeric_jp(d["å®Ÿç¾æç›Š[å††]"])
        seq = d.groupby("æ—¥", as_index=False)["å®Ÿç¾æç›Š[å††]"].sum().sort_values("æ—¥")
        seq["ç´¯è¨ˆ"] = pd.to_numeric(seq["å®Ÿç¾æç›Š[å††]"], errors="coerce").cumsum()
        seq_disp = seq.copy()
        seq_disp["å®Ÿç¾æç›Š[å††]"] = seq_disp["å®Ÿç¾æç›Š[å††]"].round(0).astype("Int64")
        seq_disp["ç´¯è¨ˆ"] = seq_disp["ç´¯è¨ˆ"].round(0).astype("Int64")
        st.dataframe(seq_disp, use_container_width=True, hide_index=True)
        download_button_df(seq_disp, "â¬‡ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆç´¯è¨ˆãƒ»æ—¥æ¬¡ï¼‰", "cumulative_daily_pl.csv")
        left,right = st.columns(2)
        with left:
            fig_bar = go.Figure([go.Bar(x=seq["æ—¥"], y=seq["å®Ÿç¾æç›Š[å††]"], name="æ—¥æ¬¡ å®Ÿç¾æç›Š")])
            fig_bar.update_layout(margin=dict(l=10,r=10,t=20,b=10), height=350, xaxis_title="æ—¥ä»˜", yaxis_title="å®Ÿç¾æç›Š[å††]")
            st.plotly_chart(fig_bar, use_container_width=True)
        with right:
            fig_line = go.Figure([go.Scatter(x=seq["æ—¥"], y=seq["ç´¯è¨ˆ"], mode="lines", name="ç´¯è¨ˆ")])
            fig_line.update_layout(margin=dict(l=10,r=10,t=20,b=10), height=350, xaxis_title="æ—¥ä»˜", yaxis_title="ç´¯è¨ˆå®Ÿç¾æç›Š[å††]")
            st.plotly_chart(fig_line, use_container_width=True)

# ---- 3) å€‹åˆ¥éŠ˜æŸ„
def per_symbol_stats(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame(columns=["éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰","éŠ˜æŸ„å","å®Ÿç¾æç›Šåˆè¨ˆ","å–å¼•å›æ•°","1å›å¹³å‡æç›Š","å‹ç‡"])
    d = normalize_symbol_cols(df.copy())
    if "å®Ÿç¾æç›Š[å††]" in d.columns: d["å®Ÿç¾æç›Š"] = to_numeric_jp(d["å®Ÿç¾æç›Š[å††]"])
    else:
        cand = next((c for c in d.columns if ("å®Ÿç¾" in str(c) and "æç›Š" in str(c))), None)
        if cand is None: 
            return pd.DataFrame(columns=["éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰","éŠ˜æŸ„å","å®Ÿç¾æç›Šåˆè¨ˆ","å–å¼•å›æ•°","1å›å¹³å‡æç›Š","å‹ç‡"])
        d["å®Ÿç¾æç›Š"] = to_numeric_jp(d[cand])
    d["win"] = d["å®Ÿç¾æç›Š"]>0
    d["group_key"] = np.where(d["code_key"].notna()&(d["code_key"]!=""), d["code_key"], "NAMEONLY::"+d["name_key"].astype(str))
    agg = d.groupby("group_key").agg({"å®Ÿç¾æç›Š":["sum","count","mean"], "win":["mean"]})
    agg.columns = ["å®Ÿç¾æç›Šåˆè¨ˆ","å–å¼•å›æ•°","1å›å¹³å‡æç›Š","å‹ç‡"]
    rep_name = d.groupby("group_key").apply(representative_name).rename("éŠ˜æŸ„å")
    code_col = d.groupby("group_key")["code_key"].first().rename("éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰")
    out = agg.join(rep_name).join(code_col).reset_index(drop=True).sort_values("å®Ÿç¾æç›Šåˆè¨ˆ", ascending=False)
    out["éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰"] = out["éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰"].fillna("â€”")
    return out

with tab3:
    st.markdown("### å€‹åˆ¥éŠ˜æŸ„ï¼ˆå‹ç‡ãƒ»å®Ÿç¾æç›Šï¼‰")
    if realized_f.empty:
        st.info("å®Ÿç¾æç›Šãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã€‚")
    else:
        sym = per_symbol_stats(realized_f)
        if not sym.empty:
            disp = sym.copy()
            if "1å›å¹³å‡æç›Š" in disp.columns: disp["1å›å¹³å‡æç›Š"] = disp["1å›å¹³å‡æç›Š"].round(0).astype("Int64")
            if "å‹ç‡" in disp.columns: disp["å‹ç‡"] = (disp["å‹ç‡"]*100).round(1).map(lambda x: f"{x:.1f}%")
            order = ["éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰","éŠ˜æŸ„å","å®Ÿç¾æç›Šåˆè¨ˆ","å–å¼•å›æ•°","1å›å¹³å‡æç›Š","å‹ç‡"]
            cols = [c for c in order if c in disp.columns] + [c for c in disp.columns if c not in order]
            st.dataframe(disp[cols], use_container_width=True, hide_index=True)
            download_button_df(disp[cols], "â¬‡ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå€‹åˆ¥éŠ˜æŸ„ï¼‰", "per_symbol_stats.csv")
        else:
            st.info("é›†è¨ˆã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

# ---- 4) ãƒ©ãƒ³ã‚­ãƒ³ã‚°
with tab4:
    st.markdown("### ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆé¸æŠæœŸé–“ï¼‰")
    if realized_f.empty:
        st.info("å®Ÿç¾æç›Šãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã€‚")
    else:
        d = normalize_symbol_cols(realized_f.copy())
        d["å®Ÿç¾æç›Š"] = to_numeric_jp(d["å®Ÿç¾æç›Š[å††]"]) if "å®Ÿç¾æç›Š[å††]" in d.columns else np.nan
        d["group_key"] = np.where(d["code_key"].notna()&(d["code_key"]!=""), d["code_key"], "NAMEONLY::"+d["name_key"].astype(str))
        by_symbol = d.groupby("group_key").agg({"å®Ÿç¾æç›Š":["count","sum","mean"]})
        by_symbol.columns = ["å–å¼•å›æ•°","å®Ÿç¾æç›Šåˆè¨ˆ","1å›å¹³å‡æç›Š"]
        rep_name = d.groupby("group_key").apply(representative_name).rename("éŠ˜æŸ„å")
        code_col = d.groupby("group_key")["code_key"].first().rename("éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰")
        out = by_symbol.join(rep_name).join(code_col).reset_index(drop=True)
        out["éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰"] = out["éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰"].fillna("â€”")
        if "1å›å¹³å‡æç›Š" in out.columns: out["1å›å¹³å‡æç›Š"] = out["1å›å¹³å‡æç›Š"].round(0).astype("Int64")
        left,right = st.columns(2)
        with left:
            sort_key = st.selectbox("ã‚½ãƒ¼ãƒˆæŒ‡æ¨™", ["å®Ÿç¾æç›Šåˆè¨ˆ","å–å¼•å›æ•°","1å›å¹³å‡æç›Š"], index=0)
        with right:
            topn = st.slider("è¡¨ç¤ºä»¶æ•°", 5, 100, 20)
        out = out.sort_values(sort_key, ascending=False).head(topn)
        order = ["éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰","éŠ˜æŸ„å","å®Ÿç¾æç›Šåˆè¨ˆ","å–å¼•å›æ•°","1å›å¹³å‡æç›Š"]
        cols = [c for c in order if c in out.columns] + [c for c in out.columns if c not in order]
        st.dataframe(out[cols], use_container_width=True, hide_index=True)
        download_button_df(out[cols], "â¬‡ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼‰", "ranking.csv")

# ---- 5) 3åˆ†è¶³ IN/OUT + VWAP/MAï¼ˆï¼‹æ—¥çµŒå…ˆç‰©ãƒ»æ—¥çµŒå¹³å‡ï¼‰
with tab5:
    st.markdown("### å€‹åˆ¥éŠ˜æŸ„ã®3åˆ†è¶³ + IN/OUTï¼ˆå½“æ—¥æŒ‡å®šï½œæ™‚åˆ»ä»˜ãç´„å®šã®ã¿ï¼‰ï¼‹ æŒ‡æ¨™ï¼ˆVWAP/MAï¼‰")

    exec_all = build_exec_table_allperiod(yakujyou_all) if not yakujyou_all.empty else pd.DataFrame()

    ohlc_min_d, ohlc_max_d = ohlc_global_date_range(ohlc_map)
    allow_ohlc_only = st.checkbox("ç´„å®šãŒç„¡ã„æ—¥ã§ã‚‚3åˆ†è¶³ã‚’è¡¨ç¤ºã™ã‚‹", value=True)
    if allow_ohlc_only and (ohlc_min_d is not None) and (ohlc_max_d is not None):
        default_d = ohlc_max_d
        sel_date = st.date_input("æ—¥ä»˜ã‚’é¸æŠ", value=default_d, min_value=ohlc_min_d, max_value=ohlc_max_d)
    else:
        if not exec_all.empty:
            dates = sorted(exec_all["exec_time"].dt.date.dropna().unique().tolist())
            if dates:
                sel_date = st.date_input("æ—¥ä»˜ã‚’é¸æŠï¼ˆIN/OUTãŒã‚ã‚‹æ—¥ï¼‰", value=dates[-1], min_value=dates[0], max_value=dates[-1])
            else:
                sel_date = st.date_input("æ—¥ä»˜ã‚’é¸æŠ", value=date.today())
        else:
            sel_date = st.date_input("æ—¥ä»˜ã‚’é¸æŠ", value=date.today())

    ohlc_code_index = build_ohlc_code_index(ohlc_map)
    all_codes_in_ohlc = sorted(ohlc_code_index.keys())
    sel_options = []
    if not exec_all.empty:
        day_exec = exec_all[exec_all["exec_time"].dt.date == sel_date]
        sel_options = sorted(day_exec["code_key"].dropna().unique().tolist())
    if not sel_options and all_codes_in_ohlc:
        sel_options = all_codes_in_ohlc
    if not sel_options:
        st.warning("è¡¨ç¤ºã§ãã‚‹éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ï¼ˆç´„å®šå±¥æ­´ or 3åˆ†è¶³ï¼‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()

    CODE_TO_NAME = build_code_to_name_map(realized, yakujyou_all)
    selected_code = st.selectbox(
        "éŠ˜æŸ„ã‚’é¸æŠ",
        options=sel_options,
        index=0,
        format_func=lambda c: f"{c}ï½œ{CODE_TO_NAME.get(str(c).upper(), 'åç§°ä¸æ˜')}"
    )
    if not selected_code: st.stop()
    code4 = str(selected_code).upper()
    disp_nm = CODE_TO_NAME.get(code4, "åç§°ä¸æ˜")

    st.markdown("#### ç´„å®šå±¥æ­´ï¼ˆå½“æ—¥ï¼‰")
    trades_tbl = build_trade_table_for_display(yakujyou_all, sel_date, code4)
    if trades_tbl.empty:
        st.info("å½“æ—¥ã®ç´„å®šå±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        st.dataframe(trades_tbl, use_container_width=True, hide_index=True)
        download_button_df(trades_tbl, "â¬‡ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆç´„å®šå±¥æ­´ãƒ»å½“æ—¥ï¼‰", f"trades_{code4}_{sel_date}.csv")

    best_key, ohlc, dbg = pick_best_ohlc_key_for_date(code4, ohlc_code_index, ohlc_map, sel_date)
    if best_key is None or ohlc is None or ohlc.empty:
        st.error(f"éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ {code4} ã®3åˆ†è¶³ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
                 "\nãƒ’ãƒ³ãƒˆï¼š3åˆ†è¶³ãƒ•ã‚¡ã‚¤ãƒ«åã®æœ«å°¾ã« `_7974` ã®ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ã‚’å«ã‚ã¦ãã ã•ã„ã€‚")
        st.stop()
    auto_key = best_key

    # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³/æ—¥ä»˜ç¯„å›²
    if getattr(ohlc["time"].dtype,"tz",None) is None:
        ohlc["time"] = pd.to_datetime(ohlc["time"], errors="coerce").dt.tz_localize(TZ)
    else:
        ohlc["time"] = ohlc["time"].dt.tz_convert(TZ)
    start_dt = pd.Timestamp(f"{sel_date} 09:00", tz=TZ)
    end_dt   = pd.Timestamp(f"{sel_date} 15:30", tz=TZ)
    ohlc_day = ohlc.loc[(ohlc["time"]>=start_dt) & (ohlc["time"]<=end_dt)].copy()
    if ohlc_day.empty:
        st.warning(f"{sel_date} ã® {code4} ã®3åˆ†è¶³ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()

    # ãƒãƒ¼ã‚«ãƒ¼
    marker_groups = {}
    skipped_price = 0
    if not exec_all.empty:
        marks = exec_all[(exec_all["exec_time"].dt.date == sel_date) & (exec_all["code_key"]==code4)].copy()
        in_window = (marks["exec_time"]>=start_dt) & (marks["exec_time"]<=end_dt)
        marks = marks.loc[in_window].copy()
        if not marks.empty:
            ohlc_idx = (ohlc_day[["time","close"]]
                        .rename(columns={"time":"ohlc_time","close":"ohlc_close"})
                        .sort_values("ohlc_time"))
            mk = marks.sort_values("exec_time").copy()
            mk["exec_time_floor"] = mk["exec_time"]
            merged = pd.merge_asof(
                mk.sort_values("exec_time_floor"),
                ohlc_idx,
                left_on="exec_time_floor",
                right_on="ohlc_time",
                direction="nearest",
                tolerance=pd.Timedelta("6min")
            )
            merged["price"] = merged["price"].fillna(merged["ohlc_close"])
            skipped_price = merged["price"].isna().sum()
            merged = merged.dropna(subset=["price"])

            for act in ["è²·å»º","å£²å»º","å£²åŸ‹","è²·åŸ‹"]:
                df_act = merged[merged["action"]==act][["exec_time","price"]].copy()
                if not df_act.empty:
                    marker_groups[act] = df_act

    st.caption(
        f"è‡ªå‹•é¸æŠ: **{code4}ï½œ{disp_nm}**ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«: {auto_key}ï¼‰ï½œ"
        "IN/OUTã¯æ™‚åˆ»ä»˜ãç´„å®šã®ã¿ã€‚ä¾¡æ ¼æ¬ æã¯OHLCè¿‘å‚ã§è£œå®Œï¼ˆÂ±6åˆ†ï¼‰"
    )

    # === è¡¨ç¤ºã‚ªãƒ—ã‚·ãƒ§ãƒ³ ===
    left, mid, right = st.columns([2,2,3])
    with left:
        tf_label = st.selectbox(
            "ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ",
            ["ãã®ã¾ã¾","6åˆ†","9åˆ†","15åˆ†"],
            index=0,
            help="å…ƒãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šç´°ã‹ã„è¶³ã¯ä½œã‚Œã¾ã›ã‚“ï¼ˆ3åˆ†â†’6/9/15åˆ†ãªã©ã«é›†ç´„ï¼‰ã€‚"
        )
    with mid:
        show_breaks = st.checkbox(
            "å–å¼•æ™‚é–“å¤–ã¨æ˜¼ä¼‘ã¿ã‚’éš ã™ï¼ˆãƒ¬ãƒ³ã‚¸ãƒ–ãƒ¬ã‚¤ã‚¯ï¼‰",
            value=True,
            help="åœŸæ—¥ãƒ»å¤œé–“ï¼ˆ15:30ã€œç¿Œ9:00ï¼‰ãƒ»æ˜¼ä¼‘ã¿ï¼ˆ11:30ã€œ12:30ï¼‰ã‚’éè¡¨ç¤ºã«ã—ã¾ã™ã€‚"
        )
    with right:
        show_lines = st.multiselect(
            "ãƒ©ã‚¤ãƒ³ã®è¡¨ç¤º",
            options=["VWAP","MA1","MA2","MA3"],
            default=[x for x in ["VWAP","MA1","MA2"] if x in ohlc_day.columns],
        )

    enlarge = st.checkbox("ğŸ” ãƒãƒ£ãƒ¼ãƒˆã‚’æ‹¡å¤§è¡¨ç¤º", value=False)
    chart_h = LARGE_CHART_HEIGHT if enlarge else MAIN_CHART_HEIGHT

    # ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ é©ç”¨
    ohlc_disp = ohlc_day.copy()
    base_min = _detect_base_interval_minutes(ohlc_disp["time"])
    tf_map = {"ãã®ã¾ã¾": None, "6åˆ†":"6min", "9åˆ†":"9min", "15åˆ†":"15min"}
    target_rule = tf_map[tf_label]
    if target_rule:
        tgt_min = int(target_rule.replace("min",""))
        if base_min is not None and tgt_min >= base_min:
            ohlc_disp = _resample_ohlc(ohlc_disp, target_rule)
        else:
            st.caption("âš  å…ƒãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šç´°ã‹ã„é–“éš”ã¯ä½œã‚Œãªã„ãŸã‚ã€ãã®ã¾ã¾è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚")

    # ==== è‰²è¨­å®šï¼ˆMA3=é’ï¼‰
    COLOR_VWAP = "#808080"    # ã‚°ãƒ¬ãƒ¼
    COLOR_MA1  = "#2ca02c"    # ç·‘
    COLOR_MA2  = "#ff7f0e"    # ã‚ªãƒ¬ãƒ³ã‚¸
    COLOR_MA3  = "#1f77b4"    # é’

    # ==== å€‹åˆ¥éŠ˜æŸ„ãƒãƒ£ãƒ¼ãƒˆï¼ˆãƒãƒ¼ã‚«ãƒ¼ä»˜ãï¼‰
    fig = go.Figure()
    fig.add_candlestick(
        x=ohlc_disp["time"], open=ohlc_disp["open"], high=ohlc_disp["high"],
        low=ohlc_disp["low"], close=ohlc_disp["close"], name="3åˆ†è¶³"
    )
    if "VWAP" in show_lines and "VWAP" in ohlc_disp.columns and ohlc_disp["VWAP"].notna().any():
        fig.add_trace(go.Scatter(x=ohlc_disp["time"], y=ohlc_disp["VWAP"], mode="lines",
                                 line=dict(color=COLOR_VWAP, width=2), name="VWAP"))
    if "MA1" in show_lines and "MA1" in ohlc_disp.columns and ohlc_disp["MA1"].notna().any():
        fig.add_trace(go.Scatter(x=ohlc_disp["time"], y=ohlc_disp["MA1"], mode="lines",
                                 line=dict(color=COLOR_MA1, width=1.8), name="MA1"))
    if "MA2" in show_lines and "MA2" in ohlc_disp.columns and ohlc_disp["MA2"].notna().any():
        fig.add_trace(go.Scatter(x=ohlc_disp["time"], y=ohlc_disp["MA2"], mode="lines",
                                 line=dict(color=COLOR_MA2, width=1.8), name="MA2"))
    if "MA3" in show_lines and "MA3" in ohlc_disp.columns and ohlc_disp["MA3"].notna().any():
        fig.add_trace(go.Scatter(x=ohlc_disp["time"], y=ohlc_disp["MA3"], mode="lines",
                                 line=dict(color=COLOR_MA3, width=1.8), name="MA3"))

    # IN/OUTï¼å»ºåŸ‹ãƒãƒ¼ã‚«ãƒ¼
    COLOR_MAP = {"è²·å»º":"#ff69b4","å£²å»º":"#1f77b4","å£²åŸ‹":"#2ca02c","è²·åŸ‹":"#ff7f0e"}
    SYMBOL_MAP = {"è²·å»º":"triangle-up","å£²å»º":"triangle-up","å£²åŸ‹":"triangle-down","è²·åŸ‹":"triangle-down"}
    TEXT_POS   = {"è²·å»º":"top center","å£²å»º":"top center","å£²åŸ‹":"bottom center","è²·åŸ‹":"bottom center"}
    for act, df_act in marker_groups.items():
        fig.add_trace(go.Scatter(
            x=df_act["exec_time"], y=df_act["price"],
            mode="markers+text",
            text=[act]*len(df_act), textposition=TEXT_POS.get(act, "top center"),
            marker_symbol=SYMBOL_MAP.get(act, "circle"),
            marker_size=10,
            marker_color=COLOR_MAP.get(act, "#444"),
            name=act,
            hovertemplate="æ™‚åˆ»=%{x|%H:%M:%S}<br>ä¾¡æ ¼=%{y:.2f}<extra>"+act+"</extra>",
        ))

    fig.update_layout(
        title=f"{sel_date} - {disp_nm} ({code4}) 3åˆ†è¶³",
        height=chart_h, xaxis_title="æ™‚åˆ»", yaxis_title="ä¾¡æ ¼",
        xaxis_rangeslider_visible=False,
        hovermode="x unified",
        margin=dict(l=10,r=10,t=30,b=10),
        xaxis=dict(tickformat="%H:%M")
    )
    # ç¯„å›²ï¼†ãƒ–ãƒ¬ã‚¤ã‚¯
    fig.add_vline(x=pd.Timestamp(f"{sel_date} 09:00", tz=TZ), line=dict(width=1, dash="dot", color="#999"))
    fig.add_vline(x=pd.Timestamp(f"{sel_date} 15:30", tz=TZ), line=dict(width=1, dash="dot", color="#999"))
    if show_breaks:
        fig.update_xaxes(rangebreaks=[
            dict(bounds=["sat","mon"]),
            dict(bounds=[15.5,9], pattern="hour"),
            dict(bounds=[11.5,12.5], pattern="hour"),
        ])
    fig.update_xaxes(range=[start_dt, end_dt])
    st.plotly_chart(fig, use_container_width=True)

    if skipped_price > 0:
        st.warning(f"ä¾¡æ ¼ã‚’è£œå®Œã§ããšãƒãƒ¼ã‚«ãƒ¼ã‚’è¡¨ç¤ºã§ããªã‹ã£ãŸç´„å®š: {skipped_price} ä»¶ï¼ˆÂ±6åˆ†ã«è¶³ãŒç„¡ã„ ç­‰ï¼‰")

    # ===== æ—¥çµŒå…ˆç‰© =====
    st.markdown("#### æ—¥çµŒå…ˆç‰©ï¼ˆOSE_NK2251!ï¼‰")
    def find_first_key_by_prefix(ohlc_map: dict, prefix: str, sel_date: date | None = None) -> str | None:
        if not ohlc_map: return None
        pfx = prefix.lower()
        cands = [k for k in ohlc_map.keys() if k.lower().startswith(pfx)]
        if not cands: return None
        def norm_t(s: pd.Series):
            return s.dt.tz_localize(TZ) if getattr(s.dtype,"tz",None) is None else s.dt.tz_convert(TZ)
        if sel_date is None:
            best = max(cands, key=lambda k: norm_t(ohlc_map[k]["time"]).max())
            return best
        s_dt, e_dt = market_time(sel_date)
        def score(k):
            t = norm_t(ohlc_map[k]["time"])
            if t.empty:
                return (3, float("inf"), float("-inf"))
            in_win   = ((t>=s_dt)&(t<=e_dt)).any()
            same_day = (t.dt.date==sel_date).any()
            sel_ts   = pd.Timestamp(sel_date, tz=TZ)
            tmin, tmax = t.min(), t.max()
            if tmax < sel_ts:
                dd = (sel_ts - tmax).total_seconds()
            elif tmin > sel_ts:
                dd = (tmin - sel_ts).total_seconds()
            else:
                dd = 0.0
            return (0 if in_win else (1 if same_day else 2), dd, -tmax.value)
        cands.sort(key=score)
        return cands[0]

    key_fut = find_first_key_by_prefix(ohlc_map, "OSE_NK2251!", sel_date)
    if key_fut:
        o = ohlc_map[key_fut].copy()
        if getattr(o["time"].dtype,"tz",None) is None: o["time"] = pd.to_datetime(o["time"], errors="coerce").dt.tz_localize(TZ)
        else: o["time"] = o["time"].dt.tz_convert(TZ)
        o_day = o.loc[(o["time"]>=start_dt)&(o["time"]<=end_dt)].copy()
        if not o_day.empty:
            o_disp = o_day.copy()
            base_min2 = _detect_base_interval_minutes(o_disp["time"])
            if target_rule:
                tgt_min = int(target_rule.replace("min",""))
                if base_min2 is not None and tgt_min >= base_min2:
                    o_disp = _resample_ohlc(o_disp, target_rule)

            fig2 = go.Figure()
            fig2.add_candlestick(x=o_disp["time"], open=o_disp["open"], high=o_disp["high"], low=o_disp["low"], close=o_disp["close"], name="3åˆ†è¶³")
            if "VWAP" in show_lines and "VWAP" in o_disp.columns and o_disp["VWAP"].notna().any():
                fig2.add_trace(go.Scatter(x=o_disp["time"], y=o_disp["VWAP"], mode="lines", line=dict(color=COLOR_VWAP, width=2), name="VWAP"))
            if "MA1" in show_lines and "MA1" in o_disp.columns and o_disp["MA1"].notna().any():
                fig2.add_trace(go.Scatter(x=o_disp["time"], y=o_disp["MA1"], mode="lines", line=dict(color=COLOR_MA1, width=1.8), name="MA1"))
            if "MA2" in show_lines and "MA2" in o_disp.columns and o_disp["MA2"].notna().any():
                fig2.add_trace(go.Scatter(x=o_disp["time"], y=o_disp["MA2"], mode="lines", line=dict(color=COLOR_MA2, width=1.8), name="MA2"))
            if "MA3" in show_lines and "MA3" in o_disp.columns and o_disp["MA3"].notna().any():
                fig2.add_trace(go.Scatter(x=o_disp["time"], y=o_disp["MA3"], mode="lines", line=dict(color=COLOR_MA3, width=1.8), name="MA3"))
            fig2.update_layout(
                height=chart_h, xaxis_title="æ™‚åˆ»", yaxis_title="ä¾¡æ ¼",
                xaxis_rangeslider_visible=False, hovermode="x unified",
                margin=dict(l=10,r=10,t=10,b=10), xaxis=dict(tickformat="%H:%M"),
                title=f"{sel_date} - OSE_NK2251!ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«: {key_fut}ï¼‰"
            )
            if show_breaks:
                fig2.update_xaxes(rangebreaks=[
                    dict(bounds=["sat","mon"]),
                    dict(bounds=[15.5,9], pattern="hour"),
                    dict(bounds=[11.5,12.5], pattern="hour"),
                ])
            fig2.update_xaxes(range=[start_dt, end_dt])
            st.plotly_chart(fig2, use_container_width=True)
        else:
            st.info("å½“æ—¥ã®å…ˆç‰©ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        st.info("å…ˆç‰©ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ 'OSE_NK2251!'ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    # ===== æ—¥çµŒå¹³å‡ =====
    st.markdown("#### æ—¥çµŒå¹³å‡ï¼ˆTVC_NI225ï¼‰")
    key_cfd = find_first_key_by_prefix(ohlc_map, "TVC_NI225", sel_date)
    if key_cfd:
        o = ohlc_map[key_cfd].copy()
        if getattr(o["time"].dtype,"tz",None) is None: o["time"] = pd.to_datetime(o["time"], errors="coerce").dt.tz_localize(TZ)
        else: o["time"] = o["time"].dt.tz_convert(TZ)
        o_day = o.loc[(o["time"]>=start_dt)&(o["time"]<=end_dt)].copy()
        if not o_day.empty:
            o_disp = o_day.copy()
            base_min3 = _detect_base_interval_minutes(o_disp["time"])
            if target_rule:
                tgt_min = int(target_rule.replace("min",""))
                if base_min3 is not None and tgt_min >= base_min3:
                    o_disp = _resample_ohlc(o_disp, target_rule)

            fig3 = go.Figure()
            fig3.add_candlestick(x=o_disp["time"], open=o_disp["open"], high=o_disp["high"], low=o_disp["low"], close=o_disp["close"], name="3åˆ†è¶³")
            if "VWAP" in show_lines and "VWAP" in o_disp.columns and o_disp["VWAP"].notna().any():
                fig3.add_trace(go.Scatter(x=o_disp["time"], y=o_disp["VWAP"], mode="lines", line=dict(color=COLOR_VWAP, width=2), name="VWAP"))
            if "MA1" in show_lines and "MA1" in o_disp.columns and o_disp["MA1"].notna().any():
                fig3.add_trace(go.Scatter(x=o_disp["time"], y=o_disp["MA1"], mode="lines", line=dict(color=COLOR_MA1, width=1.8), name="MA1"))
            if "MA2" in show_lines and "MA2" in o_disp.columns and o_disp["MA2"].notna().any():
                fig3.add_trace(go.Scatter(x=o_disp["time"], y=o_disp["MA2"], mode="lines", line=dict(color=COLOR_MA2, width=1.8), name="MA2"))
            if "MA3" in show_lines and "MA3" in o_disp.columns and o_disp["MA3"].notna().any():
                fig3.add_trace(go.Scatter(x=o_disp["time"], y=o_disp["MA3"], mode="lines", line=dict(color=COLOR_MA3, width=1.8), name="MA3"))
            fig3.update_layout(
                height=chart_h, xaxis_title="æ™‚åˆ»", yaxis_title="ä¾¡æ ¼",
                xaxis_rangeslider_visible=False, hovermode="x unified",
                margin=dict(l=10,r=10,t=10,b=10), xaxis=dict(tickformat="%H:%M"),
                title=f"{sel_date} - TVC_NI225ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«: {key_cfd}ï¼‰"
            )
            if show_breaks:
                fig3.update_xaxes(rangebreaks=[
                    dict(bounds=["sat","mon"]),
                    dict(bounds=[15.5,9], pattern="hour"),
                    dict(bounds=[11.5,12.5], pattern="hour"),
                ])
            fig3.update_xaxes(range=[start_dt, end_dt])
            st.plotly_chart(fig3, use_container_width=True)
        else:
            st.info("å½“æ—¥ã®æ—¥çµŒå¹³å‡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        st.info("æ—¥çµŒå¹³å‡ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ 'TVC_NI225'ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
